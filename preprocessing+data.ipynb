{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm,tqdm_gui\n",
    "from tqdm._tqdm_notebook import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(PATH+'review.json') as f:\n",
    "    review_data = pd.DataFrame(json.loads(line) for line in f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(PATH+'business.json') as f:\n",
    "    business_data = pd.DataFrame(json.loads(line) for line in f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rest_biz = business_data[business_data['categories'].apply(str).str.contains('Restaurants')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rest_biz = rest_biz.drop(columns=['stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rest_biz.sort_values(by='review_count',ascending=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state\n",
       "NV     949953\n",
       "AZ     837216\n",
       "ON     414411\n",
       "NC     180487\n",
       "OH     154726\n",
       "PA     143283\n",
       "QC      98978\n",
       "WI      69050\n",
       "BW      24934\n",
       "EDH     23747\n",
       "IL      22186\n",
       "SC       5981\n",
       "MLN      1101\n",
       "HLD       589\n",
       "C         168\n",
       "ELN       117\n",
       "FIF       110\n",
       "NYK       101\n",
       "WLN        87\n",
       "NY         73\n",
       "NI         58\n",
       "WA         40\n",
       "01         24\n",
       "PKN        24\n",
       "ST         24\n",
       "ESX        11\n",
       "BY         10\n",
       "KHL         7\n",
       "RCC         7\n",
       "XGL         6\n",
       "3           5\n",
       "HH          4\n",
       "CA          4\n",
       "WHT         4\n",
       "ABE         3\n",
       "ZET         3\n",
       "Name: review_count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_biz.groupby(by=['state'])['review_count'].sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessed_data = rest_biz.merge(review_data,how='inner')\n",
    "preprocessed_data = preprocessed_data[preprocessed_data['state'].isin(['NV'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>attributes</th>\n",
       "      <th>business_id</th>\n",
       "      <th>categories</th>\n",
       "      <th>city</th>\n",
       "      <th>hours</th>\n",
       "      <th>is_open</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>...</th>\n",
       "      <th>review_count</th>\n",
       "      <th>state</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3655 Las Vegas Blvd S</td>\n",
       "      <td>{'Alcohol': 'full_bar', 'HasTV': False, 'Noise...</td>\n",
       "      <td>4JNXUYY8wbaaDmk3BPzlWw</td>\n",
       "      <td>[French, Steakhouses, Breakfast &amp; Brunch, Rest...</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>{'Monday': '7:00-23:00', 'Tuesday': '7:00-23:0...</td>\n",
       "      <td>1</td>\n",
       "      <td>36.112827</td>\n",
       "      <td>-115.172581</td>\n",
       "      <td>Mon Ami Gabi</td>\n",
       "      <td>...</td>\n",
       "      <td>6979</td>\n",
       "      <td>NV</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-02-22</td>\n",
       "      <td>0</td>\n",
       "      <td>WE9eUYf5EV8AxJjl8QZRtA</td>\n",
       "      <td>5</td>\n",
       "      <td>Very chic. Although, the menu items doesnt SCR...</td>\n",
       "      <td>0</td>\n",
       "      <td>K6FpHYwcJYznoXXu8ySZHw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3655 Las Vegas Blvd S</td>\n",
       "      <td>{'Alcohol': 'full_bar', 'HasTV': False, 'Noise...</td>\n",
       "      <td>4JNXUYY8wbaaDmk3BPzlWw</td>\n",
       "      <td>[French, Steakhouses, Breakfast &amp; Brunch, Rest...</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>{'Monday': '7:00-23:00', 'Tuesday': '7:00-23:0...</td>\n",
       "      <td>1</td>\n",
       "      <td>36.112827</td>\n",
       "      <td>-115.172581</td>\n",
       "      <td>Mon Ami Gabi</td>\n",
       "      <td>...</td>\n",
       "      <td>6979</td>\n",
       "      <td>NV</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-04-15</td>\n",
       "      <td>0</td>\n",
       "      <td>7Fl41hKa0wjO3TlvHKD3lw</td>\n",
       "      <td>4</td>\n",
       "      <td>Cute french bistro with great service. I came ...</td>\n",
       "      <td>1</td>\n",
       "      <td>3SGQKsO1J-jcRIp3WNxCeA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3655 Las Vegas Blvd S</td>\n",
       "      <td>{'Alcohol': 'full_bar', 'HasTV': False, 'Noise...</td>\n",
       "      <td>4JNXUYY8wbaaDmk3BPzlWw</td>\n",
       "      <td>[French, Steakhouses, Breakfast &amp; Brunch, Rest...</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>{'Monday': '7:00-23:00', 'Tuesday': '7:00-23:0...</td>\n",
       "      <td>1</td>\n",
       "      <td>36.112827</td>\n",
       "      <td>-115.172581</td>\n",
       "      <td>Mon Ami Gabi</td>\n",
       "      <td>...</td>\n",
       "      <td>6979</td>\n",
       "      <td>NV</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-11-11</td>\n",
       "      <td>0</td>\n",
       "      <td>Jm4iOfsltS3T59puoV6r8Q</td>\n",
       "      <td>5</td>\n",
       "      <td>The food is very good, the prices fair and the...</td>\n",
       "      <td>0</td>\n",
       "      <td>LPT8XlpXlHGAp0Ri4Hu4Rw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3655 Las Vegas Blvd S</td>\n",
       "      <td>{'Alcohol': 'full_bar', 'HasTV': False, 'Noise...</td>\n",
       "      <td>4JNXUYY8wbaaDmk3BPzlWw</td>\n",
       "      <td>[French, Steakhouses, Breakfast &amp; Brunch, Rest...</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>{'Monday': '7:00-23:00', 'Tuesday': '7:00-23:0...</td>\n",
       "      <td>1</td>\n",
       "      <td>36.112827</td>\n",
       "      <td>-115.172581</td>\n",
       "      <td>Mon Ami Gabi</td>\n",
       "      <td>...</td>\n",
       "      <td>6979</td>\n",
       "      <td>NV</td>\n",
       "      <td>3</td>\n",
       "      <td>2015-12-28</td>\n",
       "      <td>2</td>\n",
       "      <td>m6lwvXPCdpKHVp05Bjkhlw</td>\n",
       "      <td>5</td>\n",
       "      <td>One of my favorite go-to brunch spots on the L...</td>\n",
       "      <td>8</td>\n",
       "      <td>3NnPbhmv_vEfPTBp2pnn9Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3655 Las Vegas Blvd S</td>\n",
       "      <td>{'Alcohol': 'full_bar', 'HasTV': False, 'Noise...</td>\n",
       "      <td>4JNXUYY8wbaaDmk3BPzlWw</td>\n",
       "      <td>[French, Steakhouses, Breakfast &amp; Brunch, Rest...</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>{'Monday': '7:00-23:00', 'Tuesday': '7:00-23:0...</td>\n",
       "      <td>1</td>\n",
       "      <td>36.112827</td>\n",
       "      <td>-115.172581</td>\n",
       "      <td>Mon Ami Gabi</td>\n",
       "      <td>...</td>\n",
       "      <td>6979</td>\n",
       "      <td>NV</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-09-16</td>\n",
       "      <td>0</td>\n",
       "      <td>nkb40j7kBe2qAci1xpyd3g</td>\n",
       "      <td>5</td>\n",
       "      <td>Simply amazing steak and frites. Got the blue ...</td>\n",
       "      <td>0</td>\n",
       "      <td>xl4rsQqpibUNhR8Jqxp4OQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 address                                         attributes  \\\n",
       "0  3655 Las Vegas Blvd S  {'Alcohol': 'full_bar', 'HasTV': False, 'Noise...   \n",
       "1  3655 Las Vegas Blvd S  {'Alcohol': 'full_bar', 'HasTV': False, 'Noise...   \n",
       "2  3655 Las Vegas Blvd S  {'Alcohol': 'full_bar', 'HasTV': False, 'Noise...   \n",
       "3  3655 Las Vegas Blvd S  {'Alcohol': 'full_bar', 'HasTV': False, 'Noise...   \n",
       "4  3655 Las Vegas Blvd S  {'Alcohol': 'full_bar', 'HasTV': False, 'Noise...   \n",
       "\n",
       "              business_id                                         categories  \\\n",
       "0  4JNXUYY8wbaaDmk3BPzlWw  [French, Steakhouses, Breakfast & Brunch, Rest...   \n",
       "1  4JNXUYY8wbaaDmk3BPzlWw  [French, Steakhouses, Breakfast & Brunch, Rest...   \n",
       "2  4JNXUYY8wbaaDmk3BPzlWw  [French, Steakhouses, Breakfast & Brunch, Rest...   \n",
       "3  4JNXUYY8wbaaDmk3BPzlWw  [French, Steakhouses, Breakfast & Brunch, Rest...   \n",
       "4  4JNXUYY8wbaaDmk3BPzlWw  [French, Steakhouses, Breakfast & Brunch, Rest...   \n",
       "\n",
       "        city                                              hours  is_open  \\\n",
       "0  Las Vegas  {'Monday': '7:00-23:00', 'Tuesday': '7:00-23:0...        1   \n",
       "1  Las Vegas  {'Monday': '7:00-23:00', 'Tuesday': '7:00-23:0...        1   \n",
       "2  Las Vegas  {'Monday': '7:00-23:00', 'Tuesday': '7:00-23:0...        1   \n",
       "3  Las Vegas  {'Monday': '7:00-23:00', 'Tuesday': '7:00-23:0...        1   \n",
       "4  Las Vegas  {'Monday': '7:00-23:00', 'Tuesday': '7:00-23:0...        1   \n",
       "\n",
       "    latitude   longitude          name           ...           review_count  \\\n",
       "0  36.112827 -115.172581  Mon Ami Gabi           ...                   6979   \n",
       "1  36.112827 -115.172581  Mon Ami Gabi           ...                   6979   \n",
       "2  36.112827 -115.172581  Mon Ami Gabi           ...                   6979   \n",
       "3  36.112827 -115.172581  Mon Ami Gabi           ...                   6979   \n",
       "4  36.112827 -115.172581  Mon Ami Gabi           ...                   6979   \n",
       "\n",
       "  state  cool        date  funny               review_id  stars  \\\n",
       "0    NV     0  2011-02-22      0  WE9eUYf5EV8AxJjl8QZRtA      5   \n",
       "1    NV     0  2015-04-15      0  7Fl41hKa0wjO3TlvHKD3lw      4   \n",
       "2    NV     0  2013-11-11      0  Jm4iOfsltS3T59puoV6r8Q      5   \n",
       "3    NV     3  2015-12-28      2  m6lwvXPCdpKHVp05Bjkhlw      5   \n",
       "4    NV     0  2015-09-16      0  nkb40j7kBe2qAci1xpyd3g      5   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  Very chic. Although, the menu items doesnt SCR...       0   \n",
       "1  Cute french bistro with great service. I came ...       1   \n",
       "2  The food is very good, the prices fair and the...       0   \n",
       "3  One of my favorite go-to brunch spots on the L...       8   \n",
       "4  Simply amazing steak and frites. Got the blue ...       0   \n",
       "\n",
       "                  user_id  \n",
       "0  K6FpHYwcJYznoXXu8ySZHw  \n",
       "1  3SGQKsO1J-jcRIp3WNxCeA  \n",
       "2  LPT8XlpXlHGAp0Ri4Hu4Rw  \n",
       "3  3NnPbhmv_vEfPTBp2pnn9Q  \n",
       "4  xl4rsQqpibUNhR8Jqxp4OQ  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_sorted = preprocessed_data.sort_values(by='date',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_20150101 = date_sorted[date_sorted['date']>'2017-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_20150101 = data_20150101.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state\n",
       "NV    126225\n",
       "Name: state, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_20150101.groupby(by=['state'])['state'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126225, 22)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_20150101.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessed_dataviz = data_20150101.loc[:,['business_id','user_id','text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The data for this analysis include restaurant reviews across two states Nevada and Arizona for the past 2.5 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessed_dataviz.to_feather('./data/preprocessed_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim import models,corpora\n",
    "from tqdm import tqdm,tqdm_gui\n",
    "from tqdm._tqdm_notebook import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessed_data = pd.read_feather('preprocessed_data')\n",
    "preprocessed_data = preprocessed_data.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Convert the reviews into sentence streams\n",
    "2. Construct a bigram model\n",
    "3. Using a bigram model, construct "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we want to create a stream of sentences. And use that to create our bigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "126225it [24:55, 84.42it/s]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp.pipe(preprocessed_data['text'],n_threads=-1)\n",
    "text_out = []\n",
    "removal = []\n",
    "for review in tqdm(doc):\n",
    "    review_sent = []\n",
    "    for sent in review.sents:\n",
    "        sentence = []\n",
    "        for token in sent:        \n",
    "            if token.is_alpha and token.is_stop == False and token.pos_ not in removal:\n",
    "                lemma = token.lemma_\n",
    "                sentence.append(lemma)\n",
    "        review_sent.append(sentence)\n",
    "    text_out.append(review_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : collecting all words and their counts\n",
      "INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO : PROGRESS: at sentence #10000, processed 57199 words and 38175 word types\n",
      "INFO : PROGRESS: at sentence #20000, processed 115696 words and 68327 word types\n",
      "INFO : PROGRESS: at sentence #30000, processed 173238 words and 94673 word types\n",
      "INFO : PROGRESS: at sentence #40000, processed 231888 words and 119857 word types\n",
      "INFO : PROGRESS: at sentence #50000, processed 291563 words and 143556 word types\n",
      "INFO : PROGRESS: at sentence #60000, processed 350225 words and 165111 word types\n",
      "INFO : PROGRESS: at sentence #70000, processed 409282 words and 186121 word types\n",
      "INFO : PROGRESS: at sentence #80000, processed 467365 words and 205914 word types\n",
      "INFO : PROGRESS: at sentence #90000, processed 526147 words and 225305 word types\n",
      "INFO : PROGRESS: at sentence #100000, processed 584434 words and 244128 word types\n",
      "INFO : PROGRESS: at sentence #110000, processed 642385 words and 262146 word types\n",
      "INFO : PROGRESS: at sentence #120000, processed 701293 words and 280394 word types\n",
      "INFO : PROGRESS: at sentence #130000, processed 759729 words and 297961 word types\n",
      "INFO : PROGRESS: at sentence #140000, processed 818794 words and 315322 word types\n",
      "INFO : PROGRESS: at sentence #150000, processed 877102 words and 331959 word types\n",
      "INFO : PROGRESS: at sentence #160000, processed 937907 words and 350109 word types\n",
      "INFO : PROGRESS: at sentence #170000, processed 997967 words and 366973 word types\n",
      "INFO : PROGRESS: at sentence #180000, processed 1058333 words and 383470 word types\n",
      "INFO : PROGRESS: at sentence #190000, processed 1116086 words and 398859 word types\n",
      "INFO : PROGRESS: at sentence #200000, processed 1174648 words and 414119 word types\n",
      "INFO : PROGRESS: at sentence #210000, processed 1233479 words and 429301 word types\n",
      "INFO : PROGRESS: at sentence #220000, processed 1291640 words and 444041 word types\n",
      "INFO : PROGRESS: at sentence #230000, processed 1350283 words and 458882 word types\n",
      "INFO : PROGRESS: at sentence #240000, processed 1408398 words and 473697 word types\n",
      "INFO : PROGRESS: at sentence #250000, processed 1468660 words and 489340 word types\n",
      "INFO : PROGRESS: at sentence #260000, processed 1527306 words and 503520 word types\n",
      "INFO : PROGRESS: at sentence #270000, processed 1585907 words and 517827 word types\n",
      "INFO : PROGRESS: at sentence #280000, processed 1645179 words and 532512 word types\n",
      "INFO : PROGRESS: at sentence #290000, processed 1703498 words and 545825 word types\n",
      "INFO : PROGRESS: at sentence #300000, processed 1759937 words and 558207 word types\n",
      "INFO : PROGRESS: at sentence #310000, processed 1818878 words and 571826 word types\n",
      "INFO : PROGRESS: at sentence #320000, processed 1876510 words and 584791 word types\n",
      "INFO : PROGRESS: at sentence #330000, processed 1935558 words and 598576 word types\n",
      "INFO : PROGRESS: at sentence #340000, processed 1993598 words and 611800 word types\n",
      "INFO : PROGRESS: at sentence #350000, processed 2051743 words and 624896 word types\n",
      "INFO : PROGRESS: at sentence #360000, processed 2112216 words and 638751 word types\n",
      "INFO : PROGRESS: at sentence #370000, processed 2170622 words and 651420 word types\n",
      "INFO : PROGRESS: at sentence #380000, processed 2230553 words and 664819 word types\n",
      "INFO : PROGRESS: at sentence #390000, processed 2291146 words and 678264 word types\n",
      "INFO : PROGRESS: at sentence #400000, processed 2349550 words and 690749 word types\n",
      "INFO : PROGRESS: at sentence #410000, processed 2406866 words and 702991 word types\n",
      "INFO : PROGRESS: at sentence #420000, processed 2466047 words and 715773 word types\n",
      "INFO : PROGRESS: at sentence #430000, processed 2525483 words and 728385 word types\n",
      "INFO : PROGRESS: at sentence #440000, processed 2583367 words and 740272 word types\n",
      "INFO : PROGRESS: at sentence #450000, processed 2642221 words and 752361 word types\n",
      "INFO : PROGRESS: at sentence #460000, processed 2702518 words and 765070 word types\n",
      "INFO : PROGRESS: at sentence #470000, processed 2760824 words and 776568 word types\n",
      "INFO : PROGRESS: at sentence #480000, processed 2819578 words and 788602 word types\n",
      "INFO : PROGRESS: at sentence #490000, processed 2879458 words and 800839 word types\n",
      "INFO : PROGRESS: at sentence #500000, processed 2938072 words and 812534 word types\n",
      "INFO : PROGRESS: at sentence #510000, processed 2995998 words and 823778 word types\n",
      "INFO : PROGRESS: at sentence #520000, processed 3054734 words and 835154 word types\n",
      "INFO : PROGRESS: at sentence #530000, processed 3113631 words and 846307 word types\n",
      "INFO : PROGRESS: at sentence #540000, processed 3171826 words and 857432 word types\n",
      "INFO : PROGRESS: at sentence #550000, processed 3229531 words and 868646 word types\n",
      "INFO : PROGRESS: at sentence #560000, processed 3287358 words and 879540 word types\n",
      "INFO : PROGRESS: at sentence #570000, processed 3345565 words and 890481 word types\n",
      "INFO : PROGRESS: at sentence #580000, processed 3403442 words and 901103 word types\n",
      "INFO : PROGRESS: at sentence #590000, processed 3462220 words and 912495 word types\n",
      "INFO : PROGRESS: at sentence #600000, processed 3522456 words and 924278 word types\n",
      "INFO : PROGRESS: at sentence #610000, processed 3580051 words and 934531 word types\n",
      "INFO : PROGRESS: at sentence #620000, processed 3639059 words and 945431 word types\n",
      "INFO : PROGRESS: at sentence #630000, processed 3696874 words and 955797 word types\n",
      "INFO : PROGRESS: at sentence #640000, processed 3756199 words and 966471 word types\n",
      "INFO : PROGRESS: at sentence #650000, processed 3812691 words and 976170 word types\n",
      "INFO : PROGRESS: at sentence #660000, processed 3871879 words and 986782 word types\n",
      "INFO : PROGRESS: at sentence #670000, processed 3930449 words and 997065 word types\n",
      "INFO : PROGRESS: at sentence #680000, processed 3988356 words and 1007096 word types\n",
      "INFO : PROGRESS: at sentence #690000, processed 4046593 words and 1017537 word types\n",
      "INFO : PROGRESS: at sentence #700000, processed 4104810 words and 1027682 word types\n",
      "INFO : PROGRESS: at sentence #710000, processed 4163404 words and 1037643 word types\n",
      "INFO : PROGRESS: at sentence #720000, processed 4221552 words and 1047912 word types\n",
      "INFO : PROGRESS: at sentence #730000, processed 4280476 words and 1058102 word types\n",
      "INFO : PROGRESS: at sentence #740000, processed 4340548 words and 1068789 word types\n",
      "INFO : PROGRESS: at sentence #750000, processed 4398881 words and 1078422 word types\n",
      "INFO : PROGRESS: at sentence #760000, processed 4458611 words and 1088414 word types\n",
      "INFO : PROGRESS: at sentence #770000, processed 4517496 words and 1098845 word types\n",
      "INFO : PROGRESS: at sentence #780000, processed 4574503 words and 1108253 word types\n",
      "INFO : PROGRESS: at sentence #790000, processed 4632082 words and 1117873 word types\n",
      "INFO : PROGRESS: at sentence #800000, processed 4691919 words and 1128148 word types\n",
      "INFO : PROGRESS: at sentence #810000, processed 4751507 words and 1138220 word types\n",
      "INFO : PROGRESS: at sentence #820000, processed 4809671 words and 1147762 word types\n",
      "INFO : PROGRESS: at sentence #830000, processed 4868123 words and 1157347 word types\n",
      "INFO : PROGRESS: at sentence #840000, processed 4926364 words and 1166409 word types\n",
      "INFO : PROGRESS: at sentence #850000, processed 4985166 words and 1176176 word types\n",
      "INFO : PROGRESS: at sentence #860000, processed 5043237 words and 1185989 word types\n",
      "INFO : PROGRESS: at sentence #870000, processed 5100882 words and 1195007 word types\n",
      "INFO : PROGRESS: at sentence #880000, processed 5158969 words and 1204222 word types\n",
      "INFO : PROGRESS: at sentence #890000, processed 5217797 words and 1213627 word types\n",
      "INFO : PROGRESS: at sentence #900000, processed 5276310 words and 1223287 word types\n",
      "INFO : PROGRESS: at sentence #910000, processed 5334064 words and 1232738 word types\n",
      "INFO : PROGRESS: at sentence #920000, processed 5392537 words and 1241915 word types\n",
      "INFO : PROGRESS: at sentence #930000, processed 5452238 words and 1251872 word types\n",
      "INFO : PROGRESS: at sentence #940000, processed 5511475 words and 1261291 word types\n",
      "INFO : PROGRESS: at sentence #950000, processed 5568522 words and 1270032 word types\n",
      "INFO : PROGRESS: at sentence #960000, processed 5627575 words and 1279005 word types\n",
      "INFO : PROGRESS: at sentence #970000, processed 5687344 words and 1288500 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: at sentence #980000, processed 5746383 words and 1298087 word types\n",
      "INFO : PROGRESS: at sentence #990000, processed 5804679 words and 1307425 word types\n",
      "INFO : PROGRESS: at sentence #1000000, processed 5865678 words and 1317433 word types\n",
      "INFO : PROGRESS: at sentence #1010000, processed 5926033 words and 1327325 word types\n",
      "INFO : PROGRESS: at sentence #1020000, processed 5984415 words and 1336298 word types\n",
      "INFO : PROGRESS: at sentence #1030000, processed 6043695 words and 1345358 word types\n",
      "INFO : PROGRESS: at sentence #1040000, processed 6102974 words and 1354640 word types\n",
      "INFO : collected 1361489 word types from a corpus of 6146670 words (unigram + bigrams) and 1047276 sentences\n",
      "INFO : using 1361489 counts as vocab in Phrases<0 vocab, min_count=100, threshold=0.5, max_vocab_size=40000000>\n"
     ]
    }
   ],
   "source": [
    "total_text = []\n",
    "for text in text_out:\n",
    "    total_text +=text\n",
    "common_terms = ['by' ,'in', 'of' ,'on' ,'or', 'to','the']\n",
    "bigram_model = Phrases(total_text,scoring='npmi',common_terms=common_terms,threshold=0.5,min_count=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Words at review level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/126225 [00:00<?, ?it/s]\u001b[A/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n",
      "\n",
      "  0%|          | 539/126225 [00:00<00:23, 5367.84it/s]\u001b[A\n",
      "  1%|          | 1092/126225 [00:00<00:22, 5444.56it/s]\u001b[A\n",
      "  1%|▏         | 1625/126225 [00:00<00:23, 5406.53it/s]\u001b[A\n",
      "  2%|▏         | 2124/126225 [00:00<00:23, 5299.37it/s]\u001b[A\n",
      "  2%|▏         | 2664/126225 [00:00<00:23, 5318.71it/s]\u001b[A\n",
      "  3%|▎         | 3227/126225 [00:00<00:22, 5370.53it/s]\u001b[A\n",
      "  3%|▎         | 3772/126225 [00:00<00:22, 5380.53it/s]\u001b[A\n",
      "  3%|▎         | 4276/126225 [00:00<00:22, 5333.14it/s]\u001b[A\n",
      "  4%|▍         | 4789/126225 [00:00<00:22, 5308.70it/s]\u001b[A\n",
      "  4%|▍         | 5309/126225 [00:01<00:22, 5296.71it/s]\u001b[A\n",
      "  5%|▍         | 5816/126225 [00:01<00:22, 5255.62it/s]\u001b[A\n",
      "  5%|▌         | 6317/126225 [00:01<00:22, 5213.55it/s]\u001b[A\n",
      "  5%|▌         | 6846/126225 [00:01<00:22, 5217.86it/s]\u001b[A\n",
      "  6%|▌         | 7364/126225 [00:01<00:22, 5216.31it/s]\u001b[A\n",
      "  6%|▌         | 7873/126225 [00:01<00:22, 5199.15it/s]\u001b[A\n",
      "  7%|▋         | 8378/126225 [00:01<00:22, 5183.42it/s]\u001b[A\n",
      "  7%|▋         | 8934/126225 [00:01<00:22, 5206.48it/s]\u001b[A\n",
      "  7%|▋         | 9453/126225 [00:01<00:22, 5177.78it/s]\u001b[A\n",
      "  8%|▊         | 9959/126225 [00:01<00:22, 5165.44it/s]\u001b[A\n",
      "  8%|▊         | 10468/126225 [00:02<00:22, 5161.35it/s]\u001b[A\n",
      "  9%|▊         | 10972/126225 [00:02<00:22, 5141.03it/s]\u001b[A\n",
      "  9%|▉         | 11467/126225 [00:02<00:22, 5131.11it/s]\u001b[A\n",
      " 10%|▉         | 12031/126225 [00:02<00:22, 5153.25it/s]\u001b[A\n",
      " 10%|▉         | 12569/126225 [00:02<00:22, 5163.00it/s]\u001b[A\n",
      " 10%|█         | 13092/126225 [00:02<00:21, 5164.28it/s]\u001b[A\n",
      " 11%|█         | 13620/126225 [00:02<00:21, 5168.66it/s]\u001b[A\n",
      " 11%|█         | 14144/126225 [00:02<00:21, 5150.56it/s]\u001b[A\n",
      " 12%|█▏        | 14653/126225 [00:02<00:21, 5142.95it/s]\u001b[A\n",
      " 12%|█▏        | 15160/126225 [00:02<00:21, 5140.80it/s]\u001b[A\n",
      " 12%|█▏        | 15665/126225 [00:03<00:21, 5125.10it/s]\u001b[A\n",
      " 13%|█▎        | 16160/126225 [00:03<00:21, 5116.26it/s]\u001b[A\n",
      " 13%|█▎        | 16652/126225 [00:03<00:21, 5098.19it/s]\u001b[A\n",
      " 14%|█▎        | 17180/126225 [00:03<00:21, 5103.40it/s]\u001b[A\n",
      " 14%|█▍        | 17698/126225 [00:03<00:21, 5105.59it/s]\u001b[A\n",
      " 14%|█▍        | 18206/126225 [00:03<00:21, 5104.96it/s]\u001b[A\n",
      " 15%|█▍        | 18710/126225 [00:03<00:21, 5098.76it/s]\u001b[A\n",
      " 15%|█▌        | 19210/126225 [00:03<00:21, 5082.13it/s]\u001b[A\n",
      " 16%|█▌        | 19696/126225 [00:03<00:20, 5073.16it/s]\u001b[A\n",
      " 16%|█▌        | 20178/126225 [00:03<00:20, 5057.28it/s]\u001b[A\n",
      " 16%|█▋        | 20651/126225 [00:04<00:20, 5045.17it/s]\u001b[A\n",
      " 17%|█▋        | 21119/126225 [00:04<00:20, 5035.90it/s]\u001b[A\n",
      " 17%|█▋        | 21606/126225 [00:04<00:20, 5032.00it/s]\u001b[A\n",
      " 18%|█▊        | 22146/126225 [00:04<00:20, 5039.89it/s]\u001b[A\n",
      " 18%|█▊        | 22667/126225 [00:04<00:20, 5043.57it/s]\u001b[A\n",
      " 18%|█▊        | 23187/126225 [00:04<00:20, 5046.51it/s]\u001b[A\n",
      " 19%|█▉        | 23694/126225 [00:04<00:20, 5042.44it/s]\u001b[A\n",
      " 19%|█▉        | 24201/126225 [00:04<00:20, 5043.28it/s]\u001b[A\n",
      " 20%|█▉        | 24703/126225 [00:04<00:20, 5041.38it/s]\u001b[A\n",
      " 20%|█▉        | 25204/126225 [00:05<00:20, 5029.01it/s]\u001b[A\n",
      " 20%|██        | 25694/126225 [00:05<00:20, 5026.26it/s]\u001b[A\n",
      " 21%|██        | 26185/126225 [00:05<00:19, 5023.94it/s]\u001b[A\n",
      " 21%|██        | 26673/126225 [00:05<00:19, 5020.06it/s]\u001b[A\n",
      " 22%|██▏       | 27189/126225 [00:05<00:19, 5022.53it/s]\u001b[A\n",
      " 22%|██▏       | 27715/126225 [00:05<00:19, 5026.93it/s]\u001b[A\n",
      " 22%|██▏       | 28240/126225 [00:05<00:19, 5030.19it/s]\u001b[A\n",
      " 23%|██▎       | 28751/126225 [00:05<00:19, 5031.80it/s]\u001b[A\n",
      " 23%|██▎       | 29262/126225 [00:05<00:19, 5028.79it/s]\u001b[A\n",
      " 24%|██▎       | 29765/126225 [00:05<00:19, 5025.01it/s]\u001b[A\n",
      " 24%|██▍       | 30262/126225 [00:06<00:19, 5022.72it/s]\u001b[A\n",
      " 24%|██▍       | 30803/126225 [00:06<00:18, 5028.78it/s]\u001b[A\n",
      " 25%|██▍       | 31311/126225 [00:06<00:18, 5027.15it/s]\u001b[A\n",
      " 25%|██▌       | 31834/126225 [00:06<00:18, 5030.51it/s]\u001b[A\n",
      " 26%|██▌       | 32350/126225 [00:06<00:18, 5032.69it/s]\u001b[A\n",
      " 26%|██▌       | 32862/126225 [00:06<00:18, 5027.56it/s]\u001b[A\n",
      " 26%|██▋       | 33362/126225 [00:06<00:18, 5021.75it/s]\u001b[A\n",
      " 27%|██▋       | 33889/126225 [00:06<00:18, 5025.33it/s]\u001b[A\n",
      " 27%|██▋       | 34414/126225 [00:06<00:18, 5028.56it/s]\u001b[A\n",
      " 28%|██▊       | 34961/126225 [00:06<00:18, 5034.53it/s]\u001b[A\n",
      " 28%|██▊       | 35518/126225 [00:07<00:17, 5042.04it/s]\u001b[A\n",
      " 29%|██▊       | 36067/126225 [00:07<00:17, 5048.71it/s]\u001b[A\n",
      " 29%|██▉       | 36603/126225 [00:07<00:17, 5047.32it/s]\u001b[A\n",
      " 29%|██▉       | 37128/126225 [00:07<00:17, 5049.53it/s]\u001b[A\n",
      " 30%|██▉       | 37652/126225 [00:07<00:17, 5049.12it/s]\u001b[A\n",
      " 30%|███       | 38169/126225 [00:07<00:17, 5048.84it/s]\u001b[A\n",
      " 31%|███       | 38683/126225 [00:07<00:17, 5050.09it/s]\u001b[A\n",
      " 31%|███       | 39196/126225 [00:07<00:17, 5046.38it/s]\u001b[A\n",
      " 31%|███▏      | 39704/126225 [00:07<00:17, 5046.70it/s]\u001b[A\n",
      " 32%|███▏      | 40216/126225 [00:07<00:17, 5047.50it/s]\u001b[A\n",
      " 32%|███▏      | 40738/126225 [00:08<00:16, 5049.60it/s]\u001b[A\n",
      " 33%|███▎      | 41249/126225 [00:08<00:16, 5047.95it/s]\u001b[A\n",
      " 33%|███▎      | 41755/126225 [00:08<00:16, 5041.06it/s]\u001b[A\n",
      " 34%|███▎      | 42288/126225 [00:08<00:16, 5044.59it/s]\u001b[A\n",
      " 34%|███▍      | 42791/126225 [00:08<00:16, 5039.54it/s]\u001b[A\n",
      " 34%|███▍      | 43283/126225 [00:08<00:16, 5035.37it/s]\u001b[A\n",
      " 35%|███▍      | 43769/126225 [00:08<00:16, 5032.95it/s]\u001b[A\n",
      " 35%|███▌      | 44324/126225 [00:08<00:16, 5038.73it/s]\u001b[A\n",
      " 36%|███▌      | 44830/126225 [00:08<00:16, 5038.40it/s]\u001b[A\n",
      " 36%|███▌      | 45335/126225 [00:09<00:16, 5037.11it/s]\u001b[A\n",
      " 36%|███▋      | 45836/126225 [00:09<00:15, 5033.66it/s]\u001b[A\n",
      " 37%|███▋      | 46329/126225 [00:09<00:15, 5029.17it/s]\u001b[A\n",
      " 37%|███▋      | 46814/126225 [00:09<00:15, 5022.87it/s]\u001b[A\n",
      " 37%|███▋      | 47315/126225 [00:09<00:15, 5022.60it/s]\u001b[A\n",
      " 38%|███▊      | 47837/126225 [00:09<00:15, 5024.68it/s]\u001b[A\n",
      " 38%|███▊      | 48360/126225 [00:09<00:15, 5026.59it/s]\u001b[A\n",
      " 39%|███▊      | 48864/126225 [00:09<00:15, 5026.55it/s]\u001b[A\n",
      " 39%|███▉      | 49366/126225 [00:09<00:15, 5025.70it/s]\u001b[A\n",
      " 40%|███▉      | 49867/126225 [00:09<00:15, 5018.65it/s]\u001b[A\n",
      " 40%|███▉      | 50408/126225 [00:10<00:15, 5022.64it/s]\u001b[A\n",
      " 40%|████      | 50908/126225 [00:10<00:14, 5021.55it/s]\u001b[A\n",
      " 41%|████      | 51415/126225 [00:10<00:14, 5021.87it/s]\u001b[A\n",
      " 41%|████      | 51916/126225 [00:10<00:14, 5021.29it/s]\u001b[A\n",
      " 42%|████▏     | 52428/126225 [00:10<00:14, 5022.15it/s]\u001b[A\n",
      " 42%|████▏     | 52953/126225 [00:10<00:14, 5024.23it/s]\u001b[A\n",
      " 42%|████▏     | 53479/126225 [00:10<00:14, 5026.35it/s]\u001b[A\n",
      " 43%|████▎     | 54008/126225 [00:10<00:14, 5028.79it/s]\u001b[A\n",
      " 43%|████▎     | 54527/126225 [00:10<00:14, 5026.50it/s]\u001b[A\n",
      " 44%|████▎     | 55035/126225 [00:10<00:14, 5027.00it/s]\u001b[A\n",
      " 44%|████▍     | 55592/126225 [00:11<00:14, 5031.77it/s]\u001b[A\n",
      " 44%|████▍     | 56119/126225 [00:11<00:13, 5033.88it/s]\u001b[A\n",
      " 45%|████▍     | 56672/126225 [00:11<00:13, 5038.41it/s]\u001b[A\n",
      " 45%|████▌     | 57205/126225 [00:11<00:13, 5040.61it/s]\u001b[A\n",
      " 46%|████▌     | 57737/126225 [00:11<00:13, 5040.88it/s]\u001b[A\n",
      " 46%|████▌     | 58262/126225 [00:11<00:13, 5036.55it/s]\u001b[A\n",
      " 47%|████▋     | 58768/126225 [00:11<00:13, 5034.60it/s]\u001b[A\n",
      " 47%|████▋     | 59267/126225 [00:11<00:13, 5031.22it/s]\u001b[A\n",
      " 47%|████▋     | 59756/126225 [00:11<00:13, 5029.60it/s]\u001b[A\n",
      " 48%|████▊     | 60264/126225 [00:11<00:13, 5029.81it/s]\u001b[A\n",
      " 48%|████▊     | 60757/126225 [00:12<00:13, 5027.44it/s]\u001b[A\n",
      " 49%|████▊     | 61296/126225 [00:12<00:12, 5030.34it/s]\u001b[A\n",
      " 49%|████▉     | 61800/126225 [00:12<00:12, 5030.34it/s]\u001b[A\n",
      " 49%|████▉     | 62325/126225 [00:12<00:12, 5031.95it/s]\u001b[A\n",
      " 50%|████▉     | 62842/126225 [00:12<00:12, 5032.98it/s]\u001b[A\n",
      " 50%|█████     | 63353/126225 [00:12<00:12, 5032.44it/s]\u001b[A\n",
      " 51%|█████     | 63860/126225 [00:12<00:12, 5032.17it/s]\u001b[A\n",
      " 51%|█████     | 64365/126225 [00:12<00:12, 5029.72it/s]\u001b[A\n",
      " 51%|█████▏    | 64925/126225 [00:12<00:12, 5034.09it/s]\u001b[A\n",
      " 52%|█████▏    | 65473/126225 [00:12<00:12, 5037.46it/s]\u001b[A\n",
      " 52%|█████▏    | 66009/126225 [00:13<00:11, 5039.88it/s]\u001b[A\n",
      " 53%|█████▎    | 66537/126225 [00:13<00:11, 5039.73it/s]\u001b[A\n",
      " 53%|█████▎    | 67057/126225 [00:13<00:11, 5039.45it/s]\u001b[A\n",
      " 54%|█████▎    | 67571/126225 [00:13<00:11, 5037.45it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 68075/126225 [00:13<00:11, 5033.41it/s]\u001b[A\n",
      " 54%|█████▍    | 68564/126225 [00:13<00:11, 5032.24it/s]\u001b[A\n",
      " 55%|█████▍    | 69073/126225 [00:13<00:11, 5032.64it/s]\u001b[A\n",
      " 55%|█████▌    | 69568/126225 [00:13<00:11, 5031.97it/s]\u001b[A\n",
      " 56%|█████▌    | 70101/126225 [00:13<00:11, 5034.08it/s]\u001b[A\n",
      " 56%|█████▌    | 70619/126225 [00:14<00:11, 5035.11it/s]\u001b[A\n",
      " 56%|█████▋    | 71129/126225 [00:14<00:10, 5035.22it/s]\u001b[A\n",
      " 57%|█████▋    | 71638/126225 [00:14<00:10, 5033.32it/s]\u001b[A\n",
      " 57%|█████▋    | 72137/126225 [00:14<00:10, 5033.03it/s]\u001b[A\n",
      " 58%|█████▊    | 72647/126225 [00:14<00:10, 5033.47it/s]\u001b[A\n",
      " 58%|█████▊    | 73149/126225 [00:14<00:10, 5033.01it/s]\u001b[A\n",
      " 58%|█████▊    | 73691/126225 [00:14<00:10, 5035.67it/s]\u001b[A\n",
      " 59%|█████▉    | 74230/126225 [00:14<00:10, 5038.24it/s]\u001b[A\n",
      " 59%|█████▉    | 74752/126225 [00:14<00:10, 5034.59it/s]\u001b[A\n",
      " 60%|█████▉    | 75254/126225 [00:14<00:10, 5033.82it/s]\u001b[A\n",
      " 60%|██████    | 75759/126225 [00:15<00:10, 5033.83it/s]\u001b[A\n",
      " 60%|██████    | 76261/126225 [00:15<00:09, 5033.70it/s]\u001b[A\n",
      " 61%|██████    | 76762/126225 [00:15<00:09, 5032.09it/s]\u001b[A\n",
      " 61%|██████    | 77264/126225 [00:15<00:09, 5032.04it/s]\u001b[A\n",
      " 62%|██████▏   | 77837/126225 [00:15<00:09, 5036.49it/s]\u001b[A\n",
      " 62%|██████▏   | 78407/126225 [00:15<00:09, 5040.74it/s]\u001b[A\n",
      " 63%|██████▎   | 78974/126225 [00:15<00:09, 5044.75it/s]\u001b[A\n",
      " 63%|██████▎   | 79519/126225 [00:15<00:09, 5047.24it/s]\u001b[A\n",
      " 63%|██████▎   | 80064/126225 [00:15<00:09, 5044.40it/s]\u001b[A\n",
      " 64%|██████▍   | 80602/126225 [00:15<00:09, 5046.49it/s]\u001b[A\n",
      " 64%|██████▍   | 81128/126225 [00:16<00:08, 5044.54it/s]\u001b[A\n",
      " 65%|██████▍   | 81640/126225 [00:16<00:08, 5043.95it/s]\u001b[A\n",
      " 65%|██████▌   | 82147/126225 [00:16<00:08, 5042.81it/s]\u001b[A\n",
      " 65%|██████▌   | 82670/126225 [00:16<00:08, 5043.89it/s]\u001b[A\n",
      " 66%|██████▌   | 83178/126225 [00:16<00:08, 5041.52it/s]\u001b[A\n",
      " 66%|██████▋   | 83677/126225 [00:16<00:08, 5041.00it/s]\u001b[A\n",
      " 67%|██████▋   | 84183/126225 [00:16<00:08, 5041.29it/s]\u001b[A\n",
      " 67%|██████▋   | 84705/126225 [00:16<00:08, 5042.37it/s]\u001b[A\n",
      " 68%|██████▊   | 85219/126225 [00:16<00:08, 5042.79it/s]\u001b[A\n",
      " 68%|██████▊   | 85742/126225 [00:16<00:08, 5043.84it/s]\u001b[A\n",
      " 68%|██████▊   | 86282/126225 [00:17<00:07, 5046.01it/s]\u001b[A\n",
      " 69%|██████▉   | 86819/126225 [00:17<00:07, 5047.91it/s]\u001b[A\n",
      " 69%|██████▉   | 87345/126225 [00:17<00:07, 5048.98it/s]\u001b[A\n",
      " 70%|██████▉   | 87880/126225 [00:17<00:07, 5050.69it/s]\u001b[A\n",
      " 70%|███████   | 88408/126225 [00:17<00:07, 5051.00it/s]\u001b[A\n",
      " 70%|███████   | 88932/126225 [00:17<00:07, 5049.77it/s]\u001b[A\n",
      " 71%|███████   | 89444/126225 [00:17<00:07, 5047.95it/s]\u001b[A\n",
      " 71%|███████▏  | 89945/126225 [00:17<00:07, 5047.35it/s]\u001b[A\n",
      " 72%|███████▏  | 90457/126225 [00:17<00:07, 5047.68it/s]\u001b[A\n",
      " 72%|███████▏  | 90992/126225 [00:18<00:06, 5049.49it/s]\u001b[A\n",
      " 72%|███████▏  | 91505/126225 [00:18<00:06, 5047.54it/s]\u001b[A\n",
      " 73%|███████▎  | 92006/126225 [00:18<00:06, 5028.27it/s]\u001b[A\n",
      " 73%|███████▎  | 92471/126225 [00:18<00:06, 5026.04it/s]\u001b[A\n",
      " 74%|███████▎  | 92947/126225 [00:18<00:06, 5024.57it/s]\u001b[A\n",
      " 74%|███████▍  | 93403/126225 [00:18<00:06, 5022.05it/s]\u001b[A\n",
      " 74%|███████▍  | 93909/126225 [00:18<00:06, 5022.21it/s]\u001b[A\n",
      " 75%|███████▍  | 94441/126225 [00:18<00:06, 5023.56it/s]\u001b[A\n",
      " 75%|███████▌  | 94970/126225 [00:18<00:06, 5024.91it/s]\u001b[A\n",
      " 76%|███████▌  | 95470/126225 [00:19<00:06, 5023.32it/s]\u001b[A\n",
      " 76%|███████▌  | 95982/126225 [00:19<00:06, 5023.74it/s]\u001b[A\n",
      " 76%|███████▋  | 96480/126225 [00:19<00:05, 5023.44it/s]\u001b[A\n",
      " 77%|███████▋  | 96978/126225 [00:19<00:05, 5020.66it/s]\u001b[A\n",
      " 77%|███████▋  | 97462/126225 [00:19<00:05, 5018.25it/s]\u001b[A\n",
      " 78%|███████▊  | 97939/126225 [00:19<00:05, 5015.88it/s]\u001b[A\n",
      " 78%|███████▊  | 98416/126225 [00:19<00:05, 5014.54it/s]\u001b[A\n",
      " 78%|███████▊  | 98914/126225 [00:19<00:05, 5014.19it/s]\u001b[A\n",
      " 79%|███████▉  | 99433/126225 [00:19<00:05, 5015.23it/s]\u001b[A\n",
      " 79%|███████▉  | 99925/126225 [00:19<00:05, 5013.65it/s]\u001b[A\n",
      " 80%|███████▉  | 100411/126225 [00:20<00:05, 5012.66it/s]\u001b[A\n",
      " 80%|███████▉  | 100919/126225 [00:20<00:05, 5012.99it/s]\u001b[A\n",
      " 80%|████████  | 101431/126225 [00:20<00:04, 5013.43it/s]\u001b[A\n",
      " 81%|████████  | 101928/126225 [00:20<00:04, 5013.18it/s]\u001b[A\n",
      " 81%|████████  | 102425/126225 [00:20<00:04, 5012.29it/s]\u001b[A\n",
      " 82%|████████▏ | 102951/126225 [00:20<00:04, 5013.49it/s]\u001b[A\n",
      " 82%|████████▏ | 103454/126225 [00:20<00:04, 5012.98it/s]\u001b[A\n",
      " 82%|████████▏ | 103983/126225 [00:20<00:04, 5014.27it/s]\u001b[A\n",
      " 83%|████████▎ | 104514/126225 [00:20<00:04, 5015.66it/s]\u001b[A\n",
      " 83%|████████▎ | 105039/126225 [00:20<00:04, 5016.74it/s]\u001b[A\n",
      " 84%|████████▎ | 105557/126225 [00:21<00:04, 5016.35it/s]\u001b[A\n",
      " 84%|████████▍ | 106068/126225 [00:21<00:04, 5015.40it/s]\u001b[A\n",
      " 84%|████████▍ | 106610/126225 [00:21<00:03, 5017.30it/s]\u001b[A\n",
      " 85%|████████▍ | 107124/126225 [00:21<00:03, 5017.27it/s]\u001b[A\n",
      " 85%|████████▌ | 107635/126225 [00:21<00:03, 5017.21it/s]\u001b[A\n",
      " 86%|████████▌ | 108143/126225 [00:21<00:03, 5015.11it/s]\u001b[A\n",
      " 86%|████████▌ | 108637/126225 [00:21<00:03, 5014.49it/s]\u001b[A\n",
      " 86%|████████▋ | 109142/126225 [00:21<00:03, 5014.57it/s]\u001b[A\n",
      " 87%|████████▋ | 109672/126225 [00:21<00:03, 5015.88it/s]\u001b[A\n",
      " 87%|████████▋ | 110178/126225 [00:21<00:03, 5015.37it/s]\u001b[A\n",
      " 88%|████████▊ | 110693/126225 [00:22<00:03, 5015.93it/s]\u001b[A\n",
      " 88%|████████▊ | 111228/126225 [00:22<00:02, 5017.41it/s]\u001b[A\n",
      " 89%|████████▊ | 111742/126225 [00:22<00:02, 5015.33it/s]\u001b[A\n",
      " 89%|████████▉ | 112240/126225 [00:22<00:02, 5013.17it/s]\u001b[A\n",
      " 89%|████████▉ | 112726/126225 [00:22<00:02, 5012.26it/s]\u001b[A\n",
      " 90%|████████▉ | 113217/126225 [00:22<00:02, 5011.73it/s]\u001b[A\n",
      " 90%|█████████ | 113704/126225 [00:22<00:02, 5010.51it/s]\u001b[A\n",
      " 90%|█████████ | 114206/126225 [00:22<00:02, 5010.52it/s]\u001b[A\n",
      " 91%|█████████ | 114753/126225 [00:22<00:02, 5012.48it/s]\u001b[A\n",
      " 91%|█████████▏| 115274/126225 [00:22<00:02, 5013.22it/s]\u001b[A\n",
      " 92%|█████████▏| 115784/126225 [00:23<00:02, 5012.62it/s]\u001b[A\n",
      " 92%|█████████▏| 116287/126225 [00:23<00:01, 5008.34it/s]\u001b[A\n",
      " 93%|█████████▎| 116776/126225 [00:23<00:01, 5006.09it/s]\u001b[A\n",
      " 93%|█████████▎| 117243/126225 [00:23<00:01, 5004.01it/s]\u001b[A\n",
      " 93%|█████████▎| 117714/126225 [00:23<00:01, 5002.71it/s]\u001b[A\n",
      " 94%|█████████▎| 118206/126225 [00:23<00:01, 5002.33it/s]\u001b[A\n",
      " 94%|█████████▍| 118679/126225 [00:23<00:01, 5001.07it/s]\u001b[A\n",
      " 94%|█████████▍| 119175/126225 [00:23<00:01, 5000.90it/s]\u001b[A\n",
      " 95%|█████████▍| 119655/126225 [00:23<00:01, 4999.21it/s]\u001b[A\n",
      " 95%|█████████▌| 120129/126225 [00:24<00:01, 4997.30it/s]\u001b[A\n",
      " 96%|█████████▌| 120598/126225 [00:24<00:01, 4994.97it/s]\u001b[A\n",
      " 96%|█████████▌| 121060/126225 [00:24<00:01, 4989.69it/s]\u001b[A\n",
      " 96%|█████████▋| 121508/126225 [00:24<00:00, 4987.59it/s]\u001b[A\n",
      " 97%|█████████▋| 121978/126225 [00:24<00:00, 4986.34it/s]\u001b[A\n",
      " 97%|█████████▋| 122443/126225 [00:24<00:00, 4984.95it/s]\u001b[A\n",
      " 97%|█████████▋| 122942/126225 [00:24<00:00, 4984.96it/s]\u001b[A\n",
      " 98%|█████████▊| 123410/126225 [00:24<00:00, 4983.28it/s]\u001b[A\n",
      " 98%|█████████▊| 123875/126225 [00:24<00:00, 4980.92it/s]\u001b[A\n",
      " 99%|█████████▊| 124334/126225 [00:24<00:00, 4979.31it/s]\u001b[A\n",
      " 99%|█████████▉| 124792/126225 [00:25<00:00, 4975.92it/s]\u001b[A\n",
      " 99%|█████████▉| 125265/126225 [00:25<00:00, 4974.96it/s]\u001b[A\n",
      "100%|█████████▉| 125758/126225 [00:25<00:00, 4974.72it/s]\u001b[A\n",
      "100%|██████████| 126225/126225 [00:25<00:00, 4973.78it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "processed_review =[]\n",
    "for text in text_out:\n",
    "    review =[]\n",
    "    for t in text:\n",
    "        review +=t\n",
    "    processed_review.append(review)\n",
    "processed_bigram_reviews = [ bigram_model[review] for review in tqdm(processed_review)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ichiza',\n",
       " 'good',\n",
       " 'the',\n",
       " 'thing',\n",
       " '-PRON-',\n",
       " 'like',\n",
       " 'wagyu',\n",
       " 'yakitori',\n",
       " '-PRON-',\n",
       " 'order',\n",
       " 'yakitori',\n",
       " 'chicken',\n",
       " 'thigh',\n",
       " 'wagyu',\n",
       " 'tongue',\n",
       " 'ok',\n",
       " 'wagyu_beef',\n",
       " 'good',\n",
       " 'pork_belly',\n",
       " 'meh',\n",
       " 'duck',\n",
       " 'meh',\n",
       " 'grill',\n",
       " 'octopus',\n",
       " 'ok',\n",
       " '-PRON-',\n",
       " 'will',\n",
       " 'come']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_bigram_reviews[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amount',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'are',\n",
       " 'around',\n",
       " 'as',\n",
       " 'at',\n",
       " 'back',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'both',\n",
       " 'bottom',\n",
       " 'but',\n",
       " 'by',\n",
       " 'ca',\n",
       " 'call',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'could',\n",
       " 'did',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doing',\n",
       " 'done',\n",
       " 'down',\n",
       " 'due',\n",
       " 'during',\n",
       " 'each',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'eleven',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'empty',\n",
       " 'enough',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'except',\n",
       " 'few',\n",
       " 'fifteen',\n",
       " 'fifty',\n",
       " 'first',\n",
       " 'five',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forty',\n",
       " 'four',\n",
       " 'from',\n",
       " 'front',\n",
       " 'full',\n",
       " 'further',\n",
       " 'get',\n",
       " 'give',\n",
       " 'go',\n",
       " 'had',\n",
       " 'has',\n",
       " 'have',\n",
       " 'he',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'however',\n",
       " 'hundred',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'indeed',\n",
       " 'into',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'keep',\n",
       " 'last',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'made',\n",
       " 'make',\n",
       " 'many',\n",
       " 'may',\n",
       " 'me',\n",
       " 'meanwhile',\n",
       " 'might',\n",
       " 'mine',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'move',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'name',\n",
       " 'namely',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'part',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'please',\n",
       " 'put',\n",
       " 'quite',\n",
       " 'rather',\n",
       " 're',\n",
       " 'really',\n",
       " 'regarding',\n",
       " 'same',\n",
       " 'say',\n",
       " 'see',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'serious',\n",
       " 'several',\n",
       " 'she',\n",
       " 'should',\n",
       " 'show',\n",
       " 'side',\n",
       " 'since',\n",
       " 'six',\n",
       " 'sixty',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhere',\n",
       " 'still',\n",
       " 'such',\n",
       " 'take',\n",
       " 'ten',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " 'third',\n",
       " 'this',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'top',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'twelve',\n",
       " 'twenty',\n",
       " 'two',\n",
       " 'under',\n",
       " 'unless',\n",
       " 'until',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'used',\n",
       " 'using',\n",
       " 'various',\n",
       " 'very',\n",
       " 'via',\n",
       " 'was',\n",
       " 'we',\n",
       " 'well',\n",
       " 'were',\n",
       " 'what',\n",
       " 'whatever',\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'whither',\n",
       " 'who',\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whom',\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'would',\n",
       " 'yet',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filtering for stop words\n",
    "from spacy.lang.en.stop_words import STOP_WORDS \n",
    "STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/126225 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 417/126225 [00:00<00:30, 4118.68it/s]\u001b[A\n",
      "  1%|          | 976/126225 [00:00<00:25, 4838.65it/s]\u001b[A\n",
      "  1%|          | 1541/126225 [00:00<00:24, 5105.13it/s]\u001b[A\n",
      "  2%|▏         | 2072/126225 [00:00<00:24, 5156.09it/s]\u001b[A\n",
      "  2%|▏         | 2646/126225 [00:00<00:23, 5270.23it/s]\u001b[A\n",
      "  3%|▎         | 3253/126225 [00:00<00:22, 5402.76it/s]\u001b[A\n",
      "  3%|▎         | 3835/126225 [00:00<00:22, 5462.48it/s]\u001b[A\n",
      "  3%|▎         | 4363/126225 [00:00<00:22, 5438.83it/s]\u001b[A\n",
      "  4%|▍         | 4908/126225 [00:00<00:22, 5439.72it/s]\u001b[A\n",
      "  4%|▍         | 5455/126225 [00:01<00:22, 5442.07it/s]\u001b[A\n",
      "  5%|▍         | 5991/126225 [00:01<00:22, 5419.83it/s]\u001b[A\n",
      "  5%|▌         | 6523/126225 [00:01<00:22, 5385.45it/s]\u001b[A\n",
      "  6%|▌         | 7118/126225 [00:01<00:21, 5424.74it/s]\u001b[A\n",
      "  6%|▌         | 7661/126225 [00:01<00:21, 5415.14it/s]\u001b[A\n",
      "  7%|▋         | 8210/126225 [00:01<00:21, 5419.62it/s]\u001b[A\n",
      "  7%|▋         | 8799/126225 [00:01<00:21, 5448.34it/s]\u001b[A\n",
      "  7%|▋         | 9355/126225 [00:01<00:21, 5435.77it/s]\u001b[A\n",
      "  8%|▊         | 9902/126225 [00:01<00:21, 5432.13it/s]\u001b[A\n",
      "  8%|▊         | 10446/126225 [00:01<00:21, 5430.73it/s]\u001b[A\n",
      "  9%|▊         | 10989/126225 [00:02<00:21, 5418.73it/s]\u001b[A\n",
      "  9%|▉         | 11525/126225 [00:02<00:21, 5412.93it/s]\u001b[A\n",
      " 10%|▉         | 12105/126225 [00:02<00:21, 5429.40it/s]\u001b[A\n",
      " 10%|█         | 12684/126225 [00:02<00:20, 5444.92it/s]\u001b[A\n",
      " 10%|█         | 13242/126225 [00:02<00:20, 5449.91it/s]\u001b[A\n",
      " 11%|█         | 13799/126225 [00:02<00:20, 5443.22it/s]\u001b[A\n",
      " 11%|█▏        | 14348/126225 [00:02<00:20, 5433.87it/s]\u001b[A\n",
      " 12%|█▏        | 14907/126225 [00:02<00:20, 5439.29it/s]\u001b[A\n",
      " 12%|█▏        | 15453/126225 [00:02<00:20, 5432.07it/s]\u001b[A\n",
      " 13%|█▎        | 15993/126225 [00:02<00:20, 5426.98it/s]\u001b[A\n",
      " 13%|█▎        | 16529/126225 [00:03<00:20, 5418.00it/s]\u001b[A\n",
      " 14%|█▎        | 17116/126225 [00:03<00:20, 5431.64it/s]\u001b[A\n",
      " 14%|█▍        | 17685/126225 [00:03<00:19, 5439.48it/s]\u001b[A\n",
      " 14%|█▍        | 18239/126225 [00:03<00:19, 5442.06it/s]\u001b[A\n",
      " 15%|█▍        | 18793/126225 [00:03<00:19, 5435.65it/s]\u001b[A\n",
      " 15%|█▌        | 19337/126225 [00:03<00:19, 5423.92it/s]\u001b[A\n",
      " 16%|█▌        | 19870/126225 [00:03<00:19, 5418.28it/s]\u001b[A\n",
      " 16%|█▌        | 20400/126225 [00:03<00:19, 5395.13it/s]\u001b[A\n",
      " 17%|█▋        | 20911/126225 [00:03<00:19, 5380.51it/s]\u001b[A\n",
      " 17%|█▋        | 21441/126225 [00:03<00:19, 5378.48it/s]\u001b[A\n",
      " 17%|█▋        | 22030/126225 [00:04<00:19, 5390.56it/s]\u001b[A\n",
      " 18%|█▊        | 22592/126225 [00:04<00:19, 5396.06it/s]\u001b[A\n",
      " 18%|█▊        | 23147/126225 [00:04<00:19, 5399.38it/s]\u001b[A\n",
      " 19%|█▉        | 23694/126225 [00:04<00:18, 5400.57it/s]\u001b[A\n",
      " 19%|█▉        | 24244/126225 [00:04<00:18, 5402.75it/s]\u001b[A\n",
      " 20%|█▉        | 24831/126225 [00:04<00:18, 5412.36it/s]\u001b[A\n",
      " 20%|██        | 25390/126225 [00:04<00:18, 5400.13it/s]\u001b[A\n",
      " 21%|██        | 25928/126225 [00:04<00:18, 5397.60it/s]\u001b[A\n",
      " 21%|██        | 26488/126225 [00:04<00:18, 5401.26it/s]\u001b[A\n",
      " 21%|██▏       | 27030/126225 [00:05<00:18, 5397.43it/s]\u001b[A\n",
      " 22%|██▏       | 27613/126225 [00:05<00:18, 5405.84it/s]\u001b[A\n",
      " 22%|██▏       | 28196/126225 [00:05<00:18, 5413.85it/s]\u001b[A\n",
      " 23%|██▎       | 28756/126225 [00:05<00:18, 5413.62it/s]\u001b[A\n",
      " 23%|██▎       | 29311/126225 [00:05<00:17, 5413.59it/s]\u001b[A\n",
      " 24%|██▎       | 29861/126225 [00:05<00:17, 5408.67it/s]\u001b[A\n",
      " 24%|██▍       | 30401/126225 [00:05<00:17, 5407.87it/s]\u001b[A\n",
      " 25%|██▍       | 30986/126225 [00:05<00:17, 5415.34it/s]\u001b[A\n",
      " 25%|██▍       | 31539/126225 [00:05<00:17, 5414.72it/s]\u001b[A\n",
      " 25%|██▌       | 32143/126225 [00:05<00:17, 5425.10it/s]\u001b[A\n",
      " 26%|██▌       | 32708/126225 [00:06<00:17, 5419.17it/s]\u001b[A\n",
      " 26%|██▋       | 33257/126225 [00:06<00:17, 5416.35it/s]\u001b[A\n",
      " 27%|██▋       | 33842/126225 [00:06<00:17, 5423.16it/s]\u001b[A\n",
      " 27%|██▋       | 34410/126225 [00:06<00:16, 5427.13it/s]\u001b[A\n",
      " 28%|██▊       | 34991/126225 [00:06<00:16, 5432.88it/s]\u001b[A\n",
      " 28%|██▊       | 35627/126225 [00:06<00:16, 5447.02it/s]\u001b[A\n",
      " 29%|██▊       | 36214/126225 [00:06<00:16, 5450.33it/s]\u001b[A\n",
      " 29%|██▉       | 36794/126225 [00:06<00:16, 5449.20it/s]\u001b[A\n",
      " 30%|██▉       | 37362/126225 [00:06<00:16, 5448.25it/s]\u001b[A\n",
      " 30%|███       | 37921/126225 [00:06<00:16, 5447.34it/s]\u001b[A\n",
      " 30%|███       | 38474/126225 [00:07<00:16, 5446.80it/s]\u001b[A\n",
      " 31%|███       | 39024/126225 [00:07<00:16, 5444.66it/s]\u001b[A\n",
      " 31%|███▏      | 39568/126225 [00:07<00:15, 5444.24it/s]\u001b[A\n",
      " 32%|███▏      | 40112/126225 [00:07<00:15, 5444.20it/s]\u001b[A\n",
      " 32%|███▏      | 40661/126225 [00:07<00:15, 5444.86it/s]\u001b[A\n",
      " 33%|███▎      | 41206/126225 [00:07<00:15, 5444.00it/s]\u001b[A\n",
      " 33%|███▎      | 41749/126225 [00:07<00:15, 5440.88it/s]\u001b[A\n",
      " 34%|███▎      | 42317/126225 [00:07<00:15, 5443.90it/s]\u001b[A\n",
      " 34%|███▍      | 42863/126225 [00:07<00:15, 5435.70it/s]\u001b[A\n",
      " 34%|███▍      | 43400/126225 [00:07<00:15, 5434.84it/s]\u001b[A\n",
      " 35%|███▍      | 43949/126225 [00:08<00:15, 5435.42it/s]\u001b[A\n",
      " 35%|███▌      | 44546/126225 [00:08<00:15, 5441.91it/s]\u001b[A\n",
      " 36%|███▌      | 45101/126225 [00:08<00:14, 5442.96it/s]\u001b[A\n",
      " 36%|███▌      | 45655/126225 [00:08<00:14, 5434.91it/s]\u001b[A\n",
      " 37%|███▋      | 46189/126225 [00:08<00:14, 5429.69it/s]\u001b[A\n",
      " 37%|███▋      | 46713/126225 [00:08<00:14, 5424.58it/s]\u001b[A\n",
      " 37%|███▋      | 47230/126225 [00:08<00:14, 5419.79it/s]\u001b[A\n",
      " 38%|███▊      | 47812/126225 [00:08<00:14, 5424.19it/s]\u001b[A\n",
      " 38%|███▊      | 48365/126225 [00:08<00:14, 5425.37it/s]\u001b[A\n",
      " 39%|███▊      | 48904/126225 [00:09<00:14, 5424.48it/s]\u001b[A\n",
      " 39%|███▉      | 49447/126225 [00:09<00:14, 5424.52it/s]\u001b[A\n",
      " 40%|███▉      | 49987/126225 [00:09<00:14, 5378.33it/s]\u001b[A\n",
      " 40%|████      | 50513/126225 [00:09<00:14, 5376.89it/s]\u001b[A\n",
      " 40%|████      | 51061/126225 [00:09<00:13, 5377.96it/s]\u001b[A\n",
      " 41%|████      | 51626/126225 [00:09<00:13, 5380.69it/s]\u001b[A\n",
      " 41%|████▏     | 52161/126225 [00:09<00:13, 5380.33it/s]\u001b[A\n",
      " 42%|████▏     | 52695/126225 [00:09<00:13, 5379.77it/s]\u001b[A\n",
      " 42%|████▏     | 53285/126225 [00:09<00:13, 5385.01it/s]\u001b[A\n",
      " 43%|████▎     | 53842/126225 [00:09<00:13, 5386.83it/s]\u001b[A\n",
      " 43%|████▎     | 54392/126225 [00:10<00:13, 5384.31it/s]\u001b[A\n",
      " 44%|████▎     | 54932/126225 [00:10<00:13, 5384.13it/s]\u001b[A\n",
      " 44%|████▍     | 55531/126225 [00:10<00:13, 5389.89it/s]\u001b[A\n",
      " 44%|████▍     | 56119/126225 [00:10<00:12, 5394.55it/s]\u001b[A\n",
      " 45%|████▍     | 56724/126225 [00:10<00:12, 5400.60it/s]\u001b[A\n",
      " 45%|████▌     | 57301/126225 [00:10<00:12, 5403.05it/s]\u001b[A\n",
      " 46%|████▌     | 57875/126225 [00:10<00:12, 5402.14it/s]\u001b[A\n",
      " 46%|████▋     | 58436/126225 [00:10<00:12, 5402.37it/s]\u001b[A\n",
      " 47%|████▋     | 58992/126225 [00:10<00:12, 5400.69it/s]\u001b[A\n",
      " 47%|████▋     | 59538/126225 [00:11<00:12, 5399.76it/s]\u001b[A\n",
      " 48%|████▊     | 60102/126225 [00:11<00:12, 5401.81it/s]\u001b[A\n",
      " 48%|████▊     | 60650/126225 [00:11<00:12, 5401.51it/s]\u001b[A\n",
      " 48%|████▊     | 61212/126225 [00:11<00:12, 5403.41it/s]\u001b[A\n",
      " 49%|████▉     | 61782/126225 [00:11<00:11, 5406.00it/s]\u001b[A\n",
      " 49%|████▉     | 62338/126225 [00:11<00:11, 5405.29it/s]\u001b[A\n",
      " 50%|████▉     | 62895/126225 [00:11<00:11, 5406.76it/s]\u001b[A\n",
      " 50%|█████     | 63457/126225 [00:11<00:11, 5408.39it/s]\u001b[A\n",
      " 51%|█████     | 64011/126225 [00:11<00:11, 5407.75it/s]\u001b[A\n",
      " 51%|█████     | 64577/126225 [00:11<00:11, 5409.85it/s]\u001b[A\n",
      " 52%|█████▏    | 65185/126225 [00:12<00:11, 5415.34it/s]\u001b[A\n",
      " 52%|█████▏    | 65757/126225 [00:12<00:11, 5417.78it/s]\u001b[A\n",
      " 53%|█████▎    | 66374/126225 [00:12<00:11, 5423.87it/s]\u001b[A\n",
      " 53%|█████▎    | 66958/126225 [00:12<00:10, 5426.07it/s]\u001b[A\n",
      " 54%|█████▎    | 67538/126225 [00:12<00:10, 5423.58it/s]\u001b[A\n",
      " 54%|█████▍    | 68098/126225 [00:12<00:10, 5419.98it/s]\u001b[A\n",
      " 54%|█████▍    | 68641/126225 [00:12<00:10, 5418.63it/s]\u001b[A\n",
      " 55%|█████▍    | 69200/126225 [00:12<00:10, 5419.91it/s]\u001b[A\n",
      " 55%|█████▌    | 69756/126225 [00:12<00:10, 5420.97it/s]\u001b[A\n",
      " 56%|█████▌    | 70317/126225 [00:12<00:10, 5422.39it/s]\u001b[A\n",
      " 56%|█████▌    | 70869/126225 [00:13<00:10, 5421.25it/s]\u001b[A\n",
      " 57%|█████▋    | 71414/126225 [00:13<00:10, 5421.25it/s]\u001b[A\n",
      " 57%|█████▋    | 71966/126225 [00:13<00:10, 5421.95it/s]\u001b[A\n",
      " 57%|█████▋    | 72512/126225 [00:13<00:09, 5421.32it/s]\u001b[A\n",
      " 58%|█████▊    | 73055/126225 [00:13<00:09, 5420.10it/s]\u001b[A\n",
      " 58%|█████▊    | 73651/126225 [00:13<00:09, 5424.00it/s]\u001b[A\n",
      " 59%|█████▉    | 74236/126225 [00:13<00:09, 5427.09it/s]\u001b[A\n",
      " 59%|█████▉    | 74800/126225 [00:13<00:09, 5424.96it/s]\u001b[A\n",
      " 60%|█████▉    | 75350/126225 [00:13<00:09, 5424.76it/s]\u001b[A\n",
      " 60%|██████    | 75897/126225 [00:13<00:09, 5423.59it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 76442/126225 [00:14<00:09, 5423.74it/s]\u001b[A\n",
      " 61%|██████    | 76984/126225 [00:14<00:09, 5421.89it/s]\u001b[A\n",
      " 61%|██████▏   | 77564/126225 [00:14<00:08, 5424.42it/s]\u001b[A\n",
      " 62%|██████▏   | 78172/126225 [00:14<00:08, 5428.44it/s]\u001b[A\n",
      " 62%|██████▏   | 78781/126225 [00:14<00:08, 5432.94it/s]\u001b[A\n",
      " 63%|██████▎   | 79358/126225 [00:14<00:08, 5433.41it/s]\u001b[A\n",
      " 63%|██████▎   | 79927/126225 [00:14<00:08, 5430.04it/s]\u001b[A\n",
      " 64%|██████▍   | 80515/126225 [00:14<00:08, 5432.98it/s]\u001b[A\n",
      " 64%|██████▍   | 81075/126225 [00:14<00:08, 5430.74it/s]\u001b[A\n",
      " 65%|██████▍   | 81621/126225 [00:15<00:08, 5429.43it/s]\u001b[A\n",
      " 65%|██████▌   | 82161/126225 [00:15<00:08, 5427.63it/s]\u001b[A\n",
      " 66%|██████▌   | 82734/126225 [00:15<00:08, 5429.46it/s]\u001b[A\n",
      " 66%|██████▌   | 83282/126225 [00:15<00:07, 5429.81it/s]\u001b[A\n",
      " 66%|██████▋   | 83828/126225 [00:15<00:07, 5428.99it/s]\u001b[A\n",
      " 67%|██████▋   | 84380/126225 [00:15<00:07, 5429.46it/s]\u001b[A\n",
      " 67%|██████▋   | 84959/126225 [00:15<00:07, 5431.69it/s]\u001b[A\n",
      " 68%|██████▊   | 85514/126225 [00:15<00:07, 5431.40it/s]\u001b[A\n",
      " 68%|██████▊   | 86103/126225 [00:15<00:07, 5434.20it/s]\u001b[A\n",
      " 69%|██████▊   | 86689/126225 [00:15<00:07, 5436.85it/s]\u001b[A\n",
      " 69%|██████▉   | 87258/126225 [00:16<00:07, 5436.18it/s]\u001b[A\n",
      " 70%|██████▉   | 87816/126225 [00:16<00:07, 5436.38it/s]\u001b[A\n",
      " 70%|███████   | 88371/126225 [00:16<00:06, 5436.59it/s]\u001b[A\n",
      " 70%|███████   | 88924/126225 [00:16<00:06, 5435.33it/s]\u001b[A\n",
      " 71%|███████   | 89468/126225 [00:16<00:06, 5433.33it/s]\u001b[A\n",
      " 71%|███████▏  | 90003/126225 [00:16<00:06, 5432.65it/s]\u001b[A\n",
      " 72%|███████▏  | 90576/126225 [00:16<00:06, 5434.40it/s]\u001b[A\n",
      " 72%|███████▏  | 91139/126225 [00:16<00:06, 5435.54it/s]\u001b[A\n",
      " 73%|███████▎  | 91703/126225 [00:16<00:06, 5436.70it/s]\u001b[A\n",
      " 73%|███████▎  | 92258/126225 [00:16<00:06, 5435.75it/s]\u001b[A\n",
      " 74%|███████▎  | 92805/126225 [00:17<00:06, 5434.79it/s]\u001b[A\n",
      " 74%|███████▍  | 93346/126225 [00:17<00:06, 5432.84it/s]\u001b[A\n",
      " 74%|███████▍  | 93898/126225 [00:17<00:05, 5433.31it/s]\u001b[A\n",
      " 75%|███████▍  | 94463/126225 [00:17<00:05, 5434.52it/s]\u001b[A\n",
      " 75%|███████▌  | 95044/126225 [00:17<00:05, 5436.62it/s]\u001b[A\n",
      " 76%|███████▌  | 95601/126225 [00:17<00:05, 5435.41it/s]\u001b[A\n",
      " 76%|███████▌  | 96148/126225 [00:17<00:05, 5434.12it/s]\u001b[A\n",
      " 77%|███████▋  | 96688/126225 [00:17<00:05, 5432.36it/s]\u001b[A\n",
      " 77%|███████▋  | 97220/126225 [00:17<00:05, 5428.57it/s]\u001b[A\n",
      " 77%|███████▋  | 97737/126225 [00:18<00:05, 5427.01it/s]\u001b[A\n",
      " 78%|███████▊  | 98254/126225 [00:18<00:05, 5425.31it/s]\u001b[A\n",
      " 78%|███████▊  | 98777/126225 [00:18<00:05, 5424.18it/s]\u001b[A\n",
      " 79%|███████▊  | 99368/126225 [00:18<00:04, 5426.81it/s]\u001b[A\n",
      " 79%|███████▉  | 99911/126225 [00:18<00:04, 5426.82it/s]\u001b[A\n",
      " 80%|███████▉  | 100452/126225 [00:18<00:04, 5426.49it/s]\u001b[A\n",
      " 80%|████████  | 101002/126225 [00:18<00:04, 5426.76it/s]\u001b[A\n",
      " 80%|████████  | 101550/126225 [00:18<00:04, 5427.04it/s]\u001b[A\n",
      " 81%|████████  | 102094/126225 [00:18<00:04, 5426.17it/s]\u001b[A\n",
      " 81%|████████▏ | 102650/126225 [00:18<00:04, 5426.79it/s]\u001b[A\n",
      " 82%|████████▏ | 103201/126225 [00:19<00:04, 5427.18it/s]\u001b[A\n",
      " 82%|████████▏ | 103773/126225 [00:19<00:04, 5428.66it/s]\u001b[A\n",
      " 83%|████████▎ | 104327/126225 [00:19<00:04, 5428.42it/s]\u001b[A\n",
      " 83%|████████▎ | 104901/126225 [00:19<00:03, 5430.07it/s]\u001b[A\n",
      " 84%|████████▎ | 105462/126225 [00:19<00:03, 5430.88it/s]\u001b[A\n",
      " 84%|████████▍ | 106019/126225 [00:19<00:03, 5429.51it/s]\u001b[A\n",
      " 84%|████████▍ | 106616/126225 [00:19<00:03, 5432.24it/s]\u001b[A\n",
      " 85%|████████▍ | 107177/126225 [00:19<00:03, 5432.28it/s]\u001b[A\n",
      " 85%|████████▌ | 107733/126225 [00:19<00:03, 5432.74it/s]\u001b[A\n",
      " 86%|████████▌ | 108288/126225 [00:19<00:03, 5433.32it/s]\u001b[A\n",
      " 86%|████████▌ | 108843/126225 [00:20<00:03, 5433.08it/s]\u001b[A\n",
      " 87%|████████▋ | 109420/126225 [00:20<00:03, 5434.73it/s]\u001b[A\n",
      " 87%|████████▋ | 109986/126225 [00:20<00:02, 5435.80it/s]\u001b[A\n",
      " 88%|████████▊ | 110547/126225 [00:20<00:02, 5435.73it/s]\u001b[A\n",
      " 88%|████████▊ | 111140/126225 [00:20<00:02, 5438.13it/s]\u001b[A\n",
      " 88%|████████▊ | 111707/126225 [00:20<00:02, 5436.82it/s]\u001b[A\n",
      " 89%|████████▉ | 112259/126225 [00:20<00:02, 5434.37it/s]\u001b[A\n",
      " 89%|████████▉ | 112795/126225 [00:20<00:02, 5432.16it/s]\u001b[A\n",
      " 90%|████████▉ | 113333/126225 [00:20<00:02, 5431.92it/s]\u001b[A\n",
      " 90%|█████████ | 113864/126225 [00:20<00:02, 5431.28it/s]\u001b[A\n",
      " 91%|█████████ | 114444/126225 [00:21<00:02, 5433.00it/s]\u001b[A\n",
      " 91%|█████████ | 115025/126225 [00:21<00:02, 5434.62it/s]\u001b[A\n",
      " 92%|█████████▏| 115581/126225 [00:21<00:01, 5435.12it/s]\u001b[A\n",
      " 92%|█████████▏| 116136/126225 [00:21<00:01, 5433.53it/s]\u001b[A\n",
      " 92%|█████████▏| 116678/126225 [00:21<00:01, 5432.08it/s]\u001b[A\n",
      " 93%|█████████▎| 117212/126225 [00:21<00:01, 5429.15it/s]\u001b[A\n",
      " 93%|█████████▎| 117732/126225 [00:21<00:01, 5428.03it/s]\u001b[A\n",
      " 94%|█████████▎| 118279/126225 [00:21<00:01, 5428.08it/s]\u001b[A\n",
      " 94%|█████████▍| 118807/126225 [00:21<00:01, 5426.82it/s]\u001b[A\n",
      " 95%|█████████▍| 119358/126225 [00:21<00:01, 5427.16it/s]\u001b[A\n",
      " 95%|█████████▍| 119890/126225 [00:22<00:01, 5423.81it/s]\u001b[A\n",
      " 95%|█████████▌| 120405/126225 [00:22<00:01, 5421.62it/s]\u001b[A\n",
      " 96%|█████████▌| 120914/126225 [00:22<00:00, 5417.18it/s]\u001b[A\n",
      " 96%|█████████▌| 121407/126225 [00:22<00:00, 5412.83it/s]\u001b[A\n",
      " 97%|█████████▋| 121909/126225 [00:22<00:00, 5411.09it/s]\u001b[A\n",
      " 97%|█████████▋| 122417/126225 [00:22<00:00, 5409.57it/s]\u001b[A\n",
      " 97%|█████████▋| 122976/126225 [00:22<00:00, 5410.32it/s]\u001b[A\n",
      " 98%|█████████▊| 123489/126225 [00:22<00:00, 5408.31it/s]\u001b[A\n",
      " 98%|█████████▊| 123997/126225 [00:22<00:00, 5405.22it/s]\u001b[A\n",
      " 99%|█████████▊| 124518/126225 [00:23<00:00, 5404.37it/s]\u001b[A\n",
      " 99%|█████████▉| 125023/126225 [00:23<00:00, 5402.23it/s]\u001b[A\n",
      " 99%|█████████▉| 125542/126225 [00:23<00:00, 5401.30it/s]\u001b[A\n",
      "100%|█████████▉| 126077/126225 [00:23<00:00, 5401.07it/s]\u001b[A\n",
      "100%|██████████| 126225/126225 [00:23<00:00, 5400.51it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "final_bigrams = []\n",
    "stopwordlist = list(STOP_WORDS)\n",
    "for r in tqdm(processed_bigram_reviews):\n",
    "    review = []\n",
    "    for word in r:\n",
    "        if word not in stopwordlist and word != '-PRON-':\n",
    "            review.append(word)\n",
    "    final_bigrams.append(review)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ichiza',\n",
       " 'good',\n",
       " 'thing',\n",
       " 'like',\n",
       " 'wagyu',\n",
       " 'yakitori',\n",
       " 'order',\n",
       " 'yakitori',\n",
       " 'chicken',\n",
       " 'thigh',\n",
       " 'wagyu',\n",
       " 'tongue',\n",
       " 'ok',\n",
       " 'wagyu_beef',\n",
       " 'good',\n",
       " 'pork_belly',\n",
       " 'meh',\n",
       " 'duck',\n",
       " 'meh',\n",
       " 'grill',\n",
       " 'octopus',\n",
       " 'ok',\n",
       " 'come']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_bigrams[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO : adding document #10000 to Dictionary(18314 unique tokens: ['amazing', 'bartender', 'busy', 'butt', 'customer']...)\n",
      "INFO : adding document #20000 to Dictionary(26863 unique tokens: ['amazing', 'bartender', 'busy', 'butt', 'customer']...)\n",
      "INFO : adding document #30000 to Dictionary(34093 unique tokens: ['amazing', 'bartender', 'busy', 'butt', 'customer']...)\n",
      "INFO : adding document #40000 to Dictionary(40726 unique tokens: ['amazing', 'bartender', 'busy', 'butt', 'customer']...)\n",
      "INFO : adding document #50000 to Dictionary(47229 unique tokens: ['amazing', 'bartender', 'busy', 'butt', 'customer']...)\n",
      "INFO : adding document #60000 to Dictionary(52963 unique tokens: ['amazing', 'bartender', 'busy', 'butt', 'customer']...)\n",
      "INFO : adding document #70000 to Dictionary(58508 unique tokens: ['amazing', 'bartender', 'busy', 'butt', 'customer']...)\n",
      "INFO : adding document #80000 to Dictionary(63699 unique tokens: ['amazing', 'bartender', 'busy', 'butt', 'customer']...)\n",
      "INFO : adding document #90000 to Dictionary(68822 unique tokens: ['amazing', 'bartender', 'busy', 'butt', 'customer']...)\n",
      "INFO : adding document #100000 to Dictionary(73790 unique tokens: ['amazing', 'bartender', 'busy', 'butt', 'customer']...)\n",
      "INFO : adding document #110000 to Dictionary(78583 unique tokens: ['amazing', 'bartender', 'busy', 'butt', 'customer']...)\n",
      "INFO : adding document #120000 to Dictionary(83315 unique tokens: ['amazing', 'bartender', 'busy', 'butt', 'customer']...)\n",
      "INFO : built Dictionary(86671 unique tokens: ['amazing', 'bartender', 'busy', 'butt', 'customer']...) from 126225 documents (total 4799619 corpus positions)\n",
      "\n",
      "  0%|          | 0/126225 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|▎         | 3462/126225 [00:00<00:03, 34257.80it/s]\u001b[A\n",
      "  5%|▌         | 6694/126225 [00:00<00:03, 33261.51it/s]\u001b[A\n",
      "  8%|▊         | 10033/126225 [00:00<00:03, 33311.55it/s]\u001b[A\n",
      " 11%|█         | 13455/126225 [00:00<00:03, 33530.49it/s]\u001b[A\n",
      " 13%|█▎        | 16691/126225 [00:00<00:03, 33293.00it/s]\u001b[A\n",
      " 16%|█▌        | 19951/126225 [00:00<00:03, 33178.59it/s]\u001b[A\n",
      " 18%|█▊        | 23230/126225 [00:00<00:03, 33127.24it/s]\u001b[A\n",
      " 21%|██        | 26501/126225 [00:00<00:03, 33071.89it/s]\u001b[A\n",
      " 24%|██▎       | 29772/126225 [00:00<00:02, 33024.73it/s]\u001b[A\n",
      " 26%|██▌       | 33093/126225 [00:01<00:02, 33044.14it/s]\u001b[A\n",
      " 29%|██▉       | 36531/126225 [00:01<00:02, 33166.62it/s]\u001b[A\n",
      " 32%|███▏      | 39817/126225 [00:01<00:02, 33143.51it/s]\u001b[A\n",
      " 34%|███▍      | 43093/126225 [00:01<00:02, 33024.88it/s]\u001b[A\n",
      " 37%|███▋      | 46368/126225 [00:01<00:02, 33004.70it/s]\u001b[A\n",
      " 39%|███▉      | 49621/126225 [00:01<00:02, 32925.36it/s]\u001b[A\n",
      " 42%|████▏     | 52888/126225 [00:01<00:02, 32908.54it/s]\u001b[A\n",
      " 45%|████▍     | 56206/126225 [00:01<00:02, 32924.56it/s]\u001b[A\n",
      " 47%|████▋     | 59481/126225 [00:01<00:02, 32916.85it/s]\u001b[A\n",
      " 50%|████▉     | 62774/126225 [00:01<00:01, 32916.87it/s]\u001b[A\n",
      " 52%|█████▏    | 66147/126225 [00:02<00:01, 32956.38it/s]\u001b[A\n",
      " 55%|█████▌    | 69451/126225 [00:02<00:01, 32931.98it/s]\u001b[A\n",
      " 58%|█████▊    | 72738/126225 [00:02<00:01, 32929.26it/s]\u001b[A\n",
      " 60%|██████    | 76076/126225 [00:02<00:01, 32950.88it/s]\u001b[A\n",
      " 63%|██████▎   | 79481/126225 [00:02<00:01, 32985.72it/s]\u001b[A\n",
      " 66%|██████▌   | 82814/126225 [00:02<00:01, 32957.70it/s]\u001b[A\n",
      " 68%|██████▊   | 86144/126225 [00:02<00:01, 32970.80it/s]\u001b[A\n",
      " 71%|███████   | 89451/126225 [00:02<00:01, 32968.80it/s]\u001b[A\n",
      " 73%|███████▎  | 92753/126225 [00:02<00:01, 32968.32it/s]\u001b[A\n",
      " 76%|███████▌  | 96054/126225 [00:02<00:00, 32967.07it/s]\u001b[A\n",
      " 79%|███████▊  | 99352/126225 [00:03<00:00, 32921.39it/s]\u001b[A\n",
      " 81%|████████▏ | 102620/126225 [00:03<00:00, 32912.73it/s]\u001b[A\n",
      " 84%|████████▍ | 105975/126225 [00:03<00:00, 32931.45it/s]\u001b[A\n",
      " 87%|████████▋ | 109379/126225 [00:03<00:00, 32966.40it/s]\u001b[A\n",
      " 89%|████████▉ | 112703/126225 [00:03<00:00, 32954.78it/s]\u001b[A\n",
      " 92%|█████████▏| 116008/126225 [00:03<00:00, 32949.70it/s]\u001b[A\n",
      " 95%|█████████▍| 119304/126225 [00:03<00:00, 32903.30it/s]\u001b[A\n",
      " 97%|█████████▋| 122552/126225 [00:03<00:00, 32807.77it/s]\u001b[A\n",
      "100%|█████████▉| 125714/126225 [00:03<00:00, 32753.45it/s]\u001b[A\n",
      "100%|██████████| 126225/126225 [00:03<00:00, 32722.21it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(final_bigrams)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in tqdm(final_bigrams)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : using symmetric alpha at 0.02\n",
      "INFO : using symmetric eta at 0.02\n",
      "INFO : using serial LDA version on this node\n",
      "INFO : running online LDA training, 50 topics, 20 passes over the supplied corpus of 126225 documents, updating every 140000 documents, evaluating every ~126225 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "INFO : training LDA model using 35 processes\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #0 = documents up to #4000/126225, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #1 = documents up to #8000/126225, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #2 = documents up to #12000/126225, outstanding queue size 3\n",
      "DEBUG : processing chunk #0 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #3 = documents up to #16000/126225, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #4 = documents up to #20000/126225, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #5 = documents up to #24000/126225, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #6 = documents up to #28000/126225, outstanding queue size 7DEBUG : processing chunk #1 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #7 = documents up to #32000/126225, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #8 = documents up to #36000/126225, outstanding queue size 9\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #9 = documents up to #40000/126225, outstanding queue size 10\n",
      "DEBUG : processing chunk #2 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #10 = documents up to #44000/126225, outstanding queue size 11\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #11 = documents up to #48000/126225, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #12 = documents up to #52000/126225, outstanding queue size 13\n",
      "DEBUG : processing chunk #3 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #13 = documents up to #56000/126225, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #14 = documents up to #60000/126225, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #15 = documents up to #64000/126225, outstanding queue size 16\n",
      "DEBUG : processing chunk #4 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #16 = documents up to #68000/126225, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #17 = documents up to #72000/126225, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #18 = documents up to #76000/126225, outstanding queue size 19\n",
      "DEBUG : processing chunk #5 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #19 = documents up to #80000/126225, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #20 = documents up to #84000/126225, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #21 = documents up to #88000/126225, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #22 = documents up to #92000/126225, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #23 = documents up to #96000/126225, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #24 = documents up to #100000/126225, outstanding queue size 25DEBUG : processing chunk #6 of 4000 documents\n",
      "\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #25 = documents up to #104000/126225, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #26 = documents up to #108000/126225, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #27 = documents up to #112000/126225, outstanding queue size 28\n",
      "DEBUG : processing chunk #7 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #28 = documents up to #116000/126225, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #29 = documents up to #120000/126225, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #30 = documents up to #124000/126225, outstanding queue size 31\n",
      "DEBUG : processing chunk #8 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #31 = documents up to #126225/126225, outstanding queue size 32\n",
      "DEBUG : processing chunk #9 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #10 of 4000 documents\n",
      "DEBUG : processing chunk #11 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #12 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #13 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #14 of 4000 documents\n",
      "DEBUG : processing chunk #15 of 4000 documents\n",
      "DEBUG : 2192/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #16 of 4000 documents\n",
      "DEBUG : 2113/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #17 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #18 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2146/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #19 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #20 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2133/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2087/4000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #21 of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : 2073/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #22 of 4000 documents\n",
      "DEBUG : 2101/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #23 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #24 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2215/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #25 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2222/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2089/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #26 of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #27 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #28 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2159/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 2145/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #29 of 4000 documents\n",
      "DEBUG : 2184/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #30 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2235/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #31 of 2225 documents\n",
      "DEBUG : performing inference on a chunk of 2225 documents\n",
      "DEBUG : 2170/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2143/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 2185/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2244/4000 documents converged within 50 iterations\n",
      "DEBUG : 2177/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2134/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : 2133/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2224/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2123/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 2145/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 1161/2225 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2108/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 2174/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2195/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2206/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2166/4000 documents converged within 50 iterations\n",
      "DEBUG : 2099/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2016/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : updating topics\n",
      "INFO : topic #32 (0.020): 0.024*\"good\" + 0.021*\"place\" + 0.020*\"time\" + 0.015*\"food\" + 0.012*\"come\" + 0.010*\"like\" + 0.010*\"service\" + 0.009*\"great\" + 0.008*\"order\" + 0.005*\"try\"\n",
      "INFO : topic #22 (0.020): 0.019*\"place\" + 0.018*\"food\" + 0.015*\"good\" + 0.012*\"come\" + 0.012*\"great\" + 0.012*\"order\" + 0.010*\"service\" + 0.008*\"time\" + 0.007*\"burger\" + 0.007*\"delicious\"\n",
      "INFO : topic #4 (0.020): 0.030*\"food\" + 0.022*\"good\" + 0.017*\"place\" + 0.013*\"great\" + 0.011*\"order\" + 0.010*\"service\" + 0.010*\"restaurant\" + 0.010*\"like\" + 0.009*\"come\" + 0.009*\"love\"\n",
      "INFO : topic #43 (0.020): 0.033*\"good\" + 0.019*\"food\" + 0.010*\"come\" + 0.010*\"order\" + 0.010*\"service\" + 0.009*\"time\" + 0.008*\"place\" + 0.007*\"like\" + 0.007*\"great\" + 0.007*\"chicken\"\n",
      "INFO : topic #15 (0.020): 0.014*\"good\" + 0.012*\"place\" + 0.011*\"food\" + 0.011*\"order\" + 0.010*\"time\" + 0.010*\"service\" + 0.009*\"like\" + 0.008*\"taste\" + 0.008*\"come\" + 0.007*\"wait\"\n",
      "INFO : topic diff=41.061779, rho=1.000000\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.870 per-word bound, 233.9 perplexity estimate based on a held-out corpus of 2225 documents with 88742 words\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #0 = documents up to #4000/126225, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #1 = documents up to #8000/126225, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #2 = documents up to #12000/126225, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #3 = documents up to #16000/126225, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #4 = documents up to #20000/126225, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #5 = documents up to #24000/126225, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #6 = documents up to #28000/126225, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #7 = documents up to #32000/126225, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #8 = documents up to #36000/126225, outstanding queue size 9\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #9 = documents up to #40000/126225, outstanding queue size 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 1, dispatched chunk #10 = documents up to #44000/126225, outstanding queue size 11\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #11 = documents up to #48000/126225, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #12 = documents up to #52000/126225, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #13 = documents up to #56000/126225, outstanding queue size 14\n",
      "DEBUG : processing chunk #0 of 4000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #14 = documents up to #60000/126225, outstanding queue size 15\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #15 = documents up to #64000/126225, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #16 = documents up to #68000/126225, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #17 = documents up to #72000/126225, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #18 = documents up to #76000/126225, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #19 = documents up to #80000/126225, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #20 = documents up to #84000/126225, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #21 = documents up to #88000/126225, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #22 = documents up to #92000/126225, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #23 = documents up to #96000/126225, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #24 = documents up to #100000/126225, outstanding queue size 25\n",
      "DEBUG : processing chunk #1 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #25 = documents up to #104000/126225, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #26 = documents up to #108000/126225, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #27 = documents up to #112000/126225, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #28 = documents up to #116000/126225, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #29 = documents up to #120000/126225, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #30 = documents up to #124000/126225, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #31 = documents up to #126225/126225, outstanding queue size 32\n",
      "DEBUG : processing chunk #2 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #3 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #4 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #7 of 4000 documents\n",
      "DEBUG : processing chunk #6 of 4000 documents\n",
      "DEBUG : processing chunk #5 of 4000 documents\n",
      "DEBUG : processing chunk #8 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #9 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #10 of 4000 documents\n",
      "DEBUG : 2665/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2548/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #11 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2540/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #12 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #13 of 4000 documents\n",
      "DEBUG : processing chunk #14 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2544/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 2538/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #15 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #16 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #17 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2566/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #18 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2579/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #19 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2611/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : 2568/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #20 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #21 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #22 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #23 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #24 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #25 of 4000 documents\n",
      "DEBUG : 2609/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2545/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #26 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #27 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2579/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #28 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2554/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #29 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2601/4000 documents converged within 50 iterations\n",
      "DEBUG : 2605/4000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #30 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #31 of 2225 documents\n",
      "DEBUG : performing inference on a chunk of 2225 documents\n",
      "DEBUG : 2620/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2595/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2569/4000 documents converged within 50 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2593/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2590/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2558/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2612/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2567/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2617/4000 documents converged within 50 iterations\n",
      "DEBUG : 2584/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 1400/2225 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2552/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2645/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2616/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2576/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2480/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2445/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : topic #42 (0.020): 0.027*\"order\" + 0.012*\"food\" + 0.011*\"like\" + 0.010*\"good\" + 0.010*\"great\" + 0.009*\"come\" + 0.008*\"ask\" + 0.007*\"drink\" + 0.007*\"service\" + 0.007*\"manager\"\n",
      "INFO : topic #33 (0.020): 0.024*\"good\" + 0.017*\"come\" + 0.015*\"food\" + 0.015*\"place\" + 0.011*\"service\" + 0.011*\"great\" + 0.010*\"wait\" + 0.010*\"time\" + 0.009*\"love\" + 0.009*\"restaurant\"\n",
      "INFO : topic #17 (0.020): 0.019*\"order\" + 0.018*\"good\" + 0.017*\"come\" + 0.016*\"food\" + 0.012*\"time\" + 0.012*\"place\" + 0.010*\"chicken\" + 0.008*\"service\" + 0.008*\"try\" + 0.007*\"eat\"\n",
      "INFO : topic #20 (0.020): 0.019*\"good\" + 0.015*\"order\" + 0.013*\"food\" + 0.010*\"time\" + 0.008*\"try\" + 0.008*\"place\" + 0.007*\"chicken\" + 0.007*\"come\" + 0.007*\"restaurant\" + 0.006*\"like\"\n",
      "INFO : topic #36 (0.020): 0.046*\"good\" + 0.037*\"great\" + 0.021*\"food\" + 0.017*\"service\" + 0.016*\"place\" + 0.014*\"love\" + 0.012*\"friendly\" + 0.011*\"time\" + 0.009*\"staff\" + 0.007*\"delicious\"\n",
      "INFO : topic diff=0.184266, rho=0.172629\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.834 per-word bound, 228.2 perplexity estimate based on a held-out corpus of 2225 documents with 88742 words\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #0 = documents up to #4000/126225, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #1 = documents up to #8000/126225, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #2 = documents up to #12000/126225, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #3 = documents up to #16000/126225, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #4 = documents up to #20000/126225, outstanding queue size 5\n",
      "DEBUG : processing chunk #0 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #5 = documents up to #24000/126225, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #6 = documents up to #28000/126225, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #7 = documents up to #32000/126225, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #8 = documents up to #36000/126225, outstanding queue size 9\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #9 = documents up to #40000/126225, outstanding queue size 10\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #10 = documents up to #44000/126225, outstanding queue size 11\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #11 = documents up to #48000/126225, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #12 = documents up to #52000/126225, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #13 = documents up to #56000/126225, outstanding queue size 14\n",
      "DEBUG : processing chunk #1 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #14 = documents up to #60000/126225, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #15 = documents up to #64000/126225, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #16 = documents up to #68000/126225, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #17 = documents up to #72000/126225, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #18 = documents up to #76000/126225, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #19 = documents up to #80000/126225, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #20 = documents up to #84000/126225, outstanding queue size 21\n",
      "DEBUG : processing chunk #2 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #21 = documents up to #88000/126225, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #22 = documents up to #92000/126225, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #23 = documents up to #96000/126225, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #24 = documents up to #100000/126225, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #25 = documents up to #104000/126225, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #26 = documents up to #108000/126225, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #27 = documents up to #112000/126225, outstanding queue size 28\n",
      "DEBUG : processing chunk #3 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #28 = documents up to #116000/126225, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #29 = documents up to #120000/126225, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #30 = documents up to #124000/126225, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #31 = documents up to #126225/126225, outstanding queue size 32\n",
      "DEBUG : processing chunk #4 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #5 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #6 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #7 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #8 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #9 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2761/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #10 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #11 of 4000 documents\n",
      "DEBUG : 2701/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2621/4000 documents converged within 50 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #12 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : 2673/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #13 of 4000 documents\n",
      "DEBUG : processing chunk #14 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #15 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2667/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #16 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2615/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #17 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #18 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2693/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #19 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #20 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2743/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #21 of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #22 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2646/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 2660/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #23 of 4000 documents\n",
      "DEBUG : processing chunk #24 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #25 of 4000 documents\n",
      "DEBUG : processing chunk #26 of 4000 documents\n",
      "DEBUG : 2664/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2703/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #27 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #28 of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2738/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2716/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #29 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2651/4000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #30 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #31 of 2225 documents\n",
      "DEBUG : performing inference on a chunk of 2225 documents\n",
      "DEBUG : 2696/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2696/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2689/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2682/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2685/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 1470/2225 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2674/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2775/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2642/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2688/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2647/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2745/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2682/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2726/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2622/4000 documents converged within 50 iterations\n",
      "DEBUG : 2662/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2568/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : topic #2 (0.020): 0.023*\"food\" + 0.017*\"good\" + 0.015*\"like\" + 0.013*\"great\" + 0.012*\"come\" + 0.010*\"time\" + 0.010*\"order\" + 0.010*\"service\" + 0.009*\"place\" + 0.006*\"eat\"\n",
      "INFO : topic #28 (0.020): 0.022*\"place\" + 0.018*\"food\" + 0.013*\"good\" + 0.013*\"like\" + 0.012*\"order\" + 0.011*\"come\" + 0.010*\"try\" + 0.010*\"eat\" + 0.009*\"great\" + 0.009*\"service\"\n",
      "INFO : topic #9 (0.020): 0.018*\"food\" + 0.018*\"great\" + 0.016*\"good\" + 0.013*\"come\" + 0.011*\"place\" + 0.010*\"order\" + 0.010*\"restaurant\" + 0.009*\"service\" + 0.008*\"amazing\" + 0.007*\"time\"\n",
      "INFO : topic #40 (0.020): 0.029*\"great\" + 0.026*\"good\" + 0.022*\"food\" + 0.014*\"service\" + 0.012*\"order\" + 0.009*\"delicious\" + 0.009*\"place\" + 0.009*\"come\" + 0.008*\"try\" + 0.008*\"time\"\n",
      "INFO : topic #46 (0.020): 0.017*\"place\" + 0.013*\"come\" + 0.012*\"good\" + 0.010*\"pizza\" + 0.009*\"love\" + 0.007*\"price\" + 0.007*\"food\" + 0.006*\"spot\" + 0.006*\"time\" + 0.006*\"try\"\n",
      "INFO : topic diff=0.199986, rho=0.170113\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.800 per-word bound, 222.8 perplexity estimate based on a held-out corpus of 2225 documents with 88742 words\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #0 = documents up to #4000/126225, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #1 = documents up to #8000/126225, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #2 = documents up to #12000/126225, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #3 = documents up to #16000/126225, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #4 = documents up to #20000/126225, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #5 = documents up to #24000/126225, outstanding queue size 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 3, dispatched chunk #6 = documents up to #28000/126225, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #7 = documents up to #32000/126225, outstanding queue size 8\n",
      "DEBUG : processing chunk #0 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #8 = documents up to #36000/126225, outstanding queue size 9\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #9 = documents up to #40000/126225, outstanding queue size 10\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #10 = documents up to #44000/126225, outstanding queue size 11\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #11 = documents up to #48000/126225, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #12 = documents up to #52000/126225, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #13 = documents up to #56000/126225, outstanding queue size 14\n",
      "DEBUG : processing chunk #1 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #14 = documents up to #60000/126225, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #15 = documents up to #64000/126225, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #16 = documents up to #68000/126225, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #17 = documents up to #72000/126225, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #18 = documents up to #76000/126225, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #19 = documents up to #80000/126225, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #20 = documents up to #84000/126225, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #21 = documents up to #88000/126225, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #22 = documents up to #92000/126225, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #23 = documents up to #96000/126225, outstanding queue size 24\n",
      "DEBUG : processing chunk #2 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #24 = documents up to #100000/126225, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #25 = documents up to #104000/126225, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #26 = documents up to #108000/126225, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #27 = documents up to #112000/126225, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #28 = documents up to #116000/126225, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #29 = documents up to #120000/126225, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #30 = documents up to #124000/126225, outstanding queue size 31DEBUG : processing chunk #3 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #31 = documents up to #126225/126225, outstanding queue size 32\n",
      "DEBUG : processing chunk #4 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #5 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #6 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #7 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2847/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #8 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2811/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #9 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2795/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #10 of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2802/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2820/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #11 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #12 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2764/4000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #13 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #14 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #15 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #16 of 4000 documents\n",
      "DEBUG : 2797/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #17 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2876/4000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #18 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #19 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #20 of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2838/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #21 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #22 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2789/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #23 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #24 of 4000 documents\n",
      "DEBUG : 2822/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #25 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2839/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #26 of 4000 documents\n",
      "DEBUG : 2875/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2810/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #27 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #28 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2849/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #29 of 4000 documents\n",
      "DEBUG : 2839/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2841/4000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #30 of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #31 of 2225 documents\n",
      "DEBUG : performing inference on a chunk of 2225 documents\n",
      "DEBUG : 2837/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2873/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2840/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 2810/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2879/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2808/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : 2854/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2797/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2811/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 1505/2225 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2834/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2849/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2831/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2706/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2777/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : topic #16 (0.020): 0.027*\"good\" + 0.018*\"food\" + 0.016*\"like\" + 0.014*\"order\" + 0.011*\"try\" + 0.011*\"chicken\" + 0.009*\"great\" + 0.009*\"come\" + 0.009*\"taste\" + 0.008*\"place\"\n",
      "INFO : topic #35 (0.020): 0.040*\"order\" + 0.016*\"time\" + 0.014*\"cheese\" + 0.014*\"food\" + 0.013*\"good\" + 0.012*\"chicken\" + 0.011*\"come\" + 0.010*\"fry\" + 0.008*\"like\" + 0.007*\"place\"\n",
      "INFO : topic #42 (0.020): 0.028*\"order\" + 0.014*\"manager\" + 0.013*\"ask\" + 0.012*\"tell\" + 0.012*\"food\" + 0.010*\"like\" + 0.010*\"come\" + 0.008*\"good\" + 0.008*\"drink\" + 0.008*\"want\"\n",
      "INFO : topic #21 (0.020): 0.014*\"good\" + 0.011*\"steak\" + 0.010*\"order\" + 0.009*\"like\" + 0.009*\"time\" + 0.008*\"place\" + 0.008*\"try\" + 0.007*\"tell\" + 0.006*\"eat\" + 0.005*\"nice\"\n",
      "INFO : topic #1 (0.020): 0.017*\"good\" + 0.016*\"place\" + 0.012*\"food\" + 0.010*\"great\" + 0.010*\"service\" + 0.008*\"order\" + 0.007*\"bar\" + 0.007*\"love\" + 0.007*\"nice\" + 0.006*\"drink\"\n",
      "INFO : topic diff=0.212482, rho=0.167703\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.765 per-word bound, 217.5 perplexity estimate based on a held-out corpus of 2225 documents with 88742 words\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #0 = documents up to #4000/126225, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #1 = documents up to #8000/126225, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #2 = documents up to #12000/126225, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #3 = documents up to #16000/126225, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #4 = documents up to #20000/126225, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #5 = documents up to #24000/126225, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #6 = documents up to #28000/126225, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #7 = documents up to #32000/126225, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #8 = documents up to #36000/126225, outstanding queue size 9\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #9 = documents up to #40000/126225, outstanding queue size 10\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #10 = documents up to #44000/126225, outstanding queue size 11\n",
      "DEBUG : processing chunk #0 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #11 = documents up to #48000/126225, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #12 = documents up to #52000/126225, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #13 = documents up to #56000/126225, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #14 = documents up to #60000/126225, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #15 = documents up to #64000/126225, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #16 = documents up to #68000/126225, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #17 = documents up to #72000/126225, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #18 = documents up to #76000/126225, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #19 = documents up to #80000/126225, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #20 = documents up to #84000/126225, outstanding queue size 21\n",
      "DEBUG : processing chunk #1 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #21 = documents up to #88000/126225, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #22 = documents up to #92000/126225, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #23 = documents up to #96000/126225, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #24 = documents up to #100000/126225, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #25 = documents up to #104000/126225, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #26 = documents up to #108000/126225, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #27 = documents up to #112000/126225, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #28 = documents up to #116000/126225, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #29 = documents up to #120000/126225, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #30 = documents up to #124000/126225, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #31 = documents up to #126225/126225, outstanding queue size 32\n",
      "DEBUG : processing chunk #2 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #3 of 4000 documents\n",
      "DEBUG : processing chunk #4 of 4000 documents\n",
      "DEBUG : processing chunk #5 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #6 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #7 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #8 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3006/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #9 of 4000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2954/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2932/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #10 of 4000 documents\n",
      "DEBUG : processing chunk #11 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2973/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #12 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : 2921/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #13 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2905/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #14 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2953/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #15 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #16 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2975/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #17 of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2978/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #18 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #19 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2885/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #20 of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2990/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #21 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2943/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #22 of 4000 documents\n",
      "DEBUG : processing chunk #23 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2951/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #24 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #25 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #26 of 4000 documents\n",
      "DEBUG : 3005/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #27 of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 2995/4000 documents converged within 50 iterations\n",
      "DEBUG : 2934/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #28 of 4000 documents\n",
      "DEBUG : 3002/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #29 of 4000 documents\n",
      "DEBUG : 2943/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #30 of 4000 documents\n",
      "DEBUG : processing chunk #31 of 2225 documents\n",
      "DEBUG : performing inference on a chunk of 2225 documents\n",
      "DEBUG : 3001/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 2975/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2945/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2966/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2992/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2980/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2933/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 1594/2225 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2998/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2997/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2926/4000 documents converged within 50 iterations\n",
      "DEBUG : 2955/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2963/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2852/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : topic #40 (0.020): 0.030*\"great\" + 0.026*\"good\" + 0.022*\"food\" + 0.014*\"service\" + 0.011*\"order\" + 0.009*\"delicious\" + 0.009*\"come\" + 0.008*\"place\" + 0.008*\"try\" + 0.008*\"time\"\n",
      "INFO : topic #13 (0.020): 0.028*\"food\" + 0.025*\"good\" + 0.014*\"place\" + 0.012*\"order\" + 0.010*\"come\" + 0.009*\"time\" + 0.009*\"delicious\" + 0.008*\"great\" + 0.008*\"fresh\" + 0.008*\"service\"\n",
      "INFO : topic #38 (0.020): 0.020*\"love\" + 0.018*\"roll\" + 0.017*\"order\" + 0.014*\"delicious\" + 0.014*\"menu\" + 0.012*\"come\" + 0.012*\"sushi\" + 0.011*\"dinner\" + 0.010*\"good\" + 0.008*\"great\"\n",
      "INFO : topic #23 (0.020): 0.021*\"food\" + 0.015*\"order\" + 0.015*\"good\" + 0.013*\"great\" + 0.012*\"place\" + 0.012*\"table\" + 0.010*\"come\" + 0.010*\"ask\" + 0.010*\"service\" + 0.009*\"wait\"\n",
      "INFO : topic #31 (0.020): 0.019*\"good\" + 0.014*\"place\" + 0.013*\"try\" + 0.013*\"food\" + 0.013*\"come\" + 0.013*\"eat\" + 0.009*\"great\" + 0.009*\"like\" + 0.008*\"wait\" + 0.007*\"time\"\n",
      "INFO : topic diff=0.218777, rho=0.165394\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.731 per-word bound, 212.4 perplexity estimate based on a held-out corpus of 2225 documents with 88742 words\n",
      "INFO : PROGRESS: pass 5, dispatched chunk #0 = documents up to #4000/126225, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 5, dispatched chunk #1 = documents up to #8000/126225, outstanding queue size 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 5, dispatched chunk #2 = documents up to #12000/126225, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 5, dispatched chunk #3 = documents up to #16000/126225, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 5, dispatched chunk #4 = documents up to #20000/126225, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 5, dispatched chunk #5 = documents up to #24000/126225, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 5, dispatched chunk #6 = documents up to #28000/126225, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 5, dispatched chunk #7 = documents up to #32000/126225, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 5, dispatched chunk #8 = documents up to #36000/126225, outstanding queue size 9\n",
      "INFO : PROGRESS: pass 5, dispatched chunk #9 = documents up to #40000/126225, outstanding queue size 10\n",
      "DEBUG : processing chunk #0 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 5, dispatched chunk #10 = documents up to #44000/126225, outstanding queue size 11\n",
      "INFO : PROGRESS: pass 5, dispatched chunk #11 = documents up to #48000/126225, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 5, dispatched chunk #12 = documents up to #52000/126225, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 5, dispatched chunk #13 = documents up to #56000/126225, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 5, dispatched chunk #14 = documents up to #60000/126225, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 5, dispatched chunk #15 = documents up to #64000/126225, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 5, dispatched chunk #16 = documents up to #68000/126225, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 5, dispatched chunk #17 = documents up to #72000/126225, outstanding queue size 18DEBUG : processing chunk #1 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "\n",
      "INFO : PROGRESS: pass 5, dispatched chunk #18 = documents up to #76000/126225, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 5, dispatched chunk #19 = documents up to #80000/126225, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 5, dispatched chunk #20 = documents up to #84000/126225, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 5, dispatched chunk #21 = documents up to #88000/126225, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 5, dispatched chunk #22 = documents up to #92000/126225, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 5, dispatched chunk #23 = documents up to #96000/126225, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 5, dispatched chunk #24 = documents up to #100000/126225, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 5, dispatched chunk #25 = documents up to #104000/126225, outstanding queue size 26\n",
      "DEBUG : processing chunk #2 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 5, dispatched chunk #26 = documents up to #108000/126225, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 5, dispatched chunk #27 = documents up to #112000/126225, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 5, dispatched chunk #28 = documents up to #116000/126225, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 5, dispatched chunk #29 = documents up to #120000/126225, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 5, dispatched chunk #30 = documents up to #124000/126225, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 5, dispatched chunk #31 = documents up to #126225/126225, outstanding queue size 32\n",
      "DEBUG : processing chunk #3 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #4 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #5 of 4000 documents\n",
      "DEBUG : processing chunk #6 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #7 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #8 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #9 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3117/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #10 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #11 of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3088/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3065/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3072/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #12 of 4000 documents\n",
      "DEBUG : processing chunk #13 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #14 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3036/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 3027/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #15 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #16 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #17 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3052/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #18 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3113/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #19 of 4000 documents\n",
      "DEBUG : 3034/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #20 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #21 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 3114/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #22 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #23 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3058/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #24 of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3051/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #25 of 4000 documents\n",
      "DEBUG : processing chunk #26 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3062/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #27 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3095/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #28 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3061/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #29 of 4000 documents\n",
      "DEBUG : processing chunk #30 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3116/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 3063/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #31 of 2225 documents\n",
      "DEBUG : performing inference on a chunk of 2225 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3059/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3075/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3070/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3053/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3080/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : 3083/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3062/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3035/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 1665/2225 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3079/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3078/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3116/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3037/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3023/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2999/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : topic #38 (0.020): 0.021*\"roll\" + 0.020*\"love\" + 0.017*\"order\" + 0.015*\"delicious\" + 0.015*\"menu\" + 0.014*\"sushi\" + 0.012*\"come\" + 0.012*\"dinner\" + 0.010*\"good\" + 0.008*\"great\"\n",
      "INFO : topic #27 (0.020): 0.031*\"good\" + 0.028*\"taco\" + 0.021*\"food\" + 0.015*\"place\" + 0.012*\"try\" + 0.010*\"restaurant\" + 0.010*\"order\" + 0.009*\"time\" + 0.008*\"like\" + 0.007*\"love\"\n",
      "INFO : topic #44 (0.020): 0.028*\"chicken\" + 0.024*\"good\" + 0.018*\"waffle\" + 0.017*\"fry\" + 0.016*\"come\" + 0.016*\"order\" + 0.013*\"food\" + 0.010*\"service\" + 0.009*\"time\" + 0.008*\"nice\"\n",
      "INFO : topic #46 (0.020): 0.018*\"place\" + 0.012*\"come\" + 0.011*\"good\" + 0.009*\"pizza\" + 0.009*\"love\" + 0.008*\"u\" + 0.008*\"price\" + 0.007*\"taco_bell\" + 0.006*\"spot\" + 0.006*\"game\"\n",
      "INFO : topic #33 (0.020): 0.023*\"good\" + 0.018*\"come\" + 0.017*\"breakfast\" + 0.016*\"food\" + 0.015*\"place\" + 0.014*\"wait\" + 0.012*\"service\" + 0.010*\"great\" + 0.010*\"time\" + 0.008*\"restaurant\"\n",
      "INFO : topic diff=0.221296, rho=0.163177\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.703 per-word bound, 208.4 perplexity estimate based on a held-out corpus of 2225 documents with 88742 words\n",
      "INFO : PROGRESS: pass 6, dispatched chunk #0 = documents up to #4000/126225, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 6, dispatched chunk #1 = documents up to #8000/126225, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 6, dispatched chunk #2 = documents up to #12000/126225, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 6, dispatched chunk #3 = documents up to #16000/126225, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 6, dispatched chunk #4 = documents up to #20000/126225, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 6, dispatched chunk #5 = documents up to #24000/126225, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 6, dispatched chunk #6 = documents up to #28000/126225, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 6, dispatched chunk #7 = documents up to #32000/126225, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 6, dispatched chunk #8 = documents up to #36000/126225, outstanding queue size 9\n",
      "INFO : PROGRESS: pass 6, dispatched chunk #9 = documents up to #40000/126225, outstanding queue size 10\n",
      "INFO : PROGRESS: pass 6, dispatched chunk #10 = documents up to #44000/126225, outstanding queue size 11\n",
      "DEBUG : processing chunk #0 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 6, dispatched chunk #11 = documents up to #48000/126225, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 6, dispatched chunk #12 = documents up to #52000/126225, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 6, dispatched chunk #13 = documents up to #56000/126225, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 6, dispatched chunk #14 = documents up to #60000/126225, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 6, dispatched chunk #15 = documents up to #64000/126225, outstanding queue size 16\n",
      "DEBUG : processing chunk #1 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 6, dispatched chunk #16 = documents up to #68000/126225, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 6, dispatched chunk #17 = documents up to #72000/126225, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 6, dispatched chunk #18 = documents up to #76000/126225, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 6, dispatched chunk #19 = documents up to #80000/126225, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 6, dispatched chunk #20 = documents up to #84000/126225, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 6, dispatched chunk #21 = documents up to #88000/126225, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 6, dispatched chunk #22 = documents up to #92000/126225, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 6, dispatched chunk #23 = documents up to #96000/126225, outstanding queue size 24\n",
      "DEBUG : processing chunk #2 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 6, dispatched chunk #24 = documents up to #100000/126225, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 6, dispatched chunk #25 = documents up to #104000/126225, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 6, dispatched chunk #26 = documents up to #108000/126225, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 6, dispatched chunk #27 = documents up to #112000/126225, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 6, dispatched chunk #28 = documents up to #116000/126225, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 6, dispatched chunk #29 = documents up to #120000/126225, outstanding queue size 30DEBUG : processing chunk #3 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "\n",
      "INFO : PROGRESS: pass 6, dispatched chunk #30 = documents up to #124000/126225, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 6, dispatched chunk #31 = documents up to #126225/126225, outstanding queue size 32\n",
      "DEBUG : processing chunk #4 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #5 of 4000 documents\n",
      "DEBUG : processing chunk #6 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #7 of 4000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3200/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #8 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #9 of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3131/4000 documents converged within 50 iterations\n",
      "DEBUG : 3114/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3151/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #10 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #11 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3127/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #12 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #13 of 4000 documents\n",
      "DEBUG : 3114/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3145/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #14 of 4000 documents\n",
      "DEBUG : processing chunk #15 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3166/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #16 of 4000 documents\n",
      "DEBUG : processing chunk #17 of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #18 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3171/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #19 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #20 of 4000 documents\n",
      "DEBUG : 3130/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3112/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3137/4000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #21 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #22 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3159/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #23 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3176/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #24 of 4000 documents\n",
      "DEBUG : processing chunk #25 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3168/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3118/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #26 of 4000 documents\n",
      "DEBUG : processing chunk #27 of 4000 documents\n",
      "DEBUG : 3177/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3189/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #28 of 4000 documents\n",
      "DEBUG : processing chunk #29 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #30 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3166/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #31 of 2225 documents\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 2225 documents\n",
      "DEBUG : 3154/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3189/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3203/4000 documents converged within 50 iterations\n",
      "DEBUG : 3152/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 3111/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3160/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3162/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 1687/2225 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3183/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3216/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3186/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3134/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3104/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : topic #33 (0.020): 0.023*\"good\" + 0.019*\"breakfast\" + 0.018*\"come\" + 0.016*\"food\" + 0.015*\"place\" + 0.014*\"wait\" + 0.011*\"service\" + 0.010*\"great\" + 0.010*\"time\" + 0.008*\"restaurant\"\n",
      "INFO : topic #9 (0.020): 0.018*\"food\" + 0.017*\"great\" + 0.015*\"good\" + 0.013*\"come\" + 0.011*\"place\" + 0.010*\"restaurant\" + 0.009*\"order\" + 0.008*\"amazing\" + 0.008*\"service\" + 0.006*\"time\"\n",
      "INFO : topic #19 (0.020): 0.040*\"burger\" + 0.023*\"place\" + 0.022*\"good\" + 0.014*\"come\" + 0.014*\"fry\" + 0.013*\"like\" + 0.012*\"order\" + 0.010*\"food\" + 0.009*\"fish\" + 0.009*\"great\"\n",
      "INFO : topic #29 (0.020): 0.023*\"burger\" + 0.016*\"fry\" + 0.016*\"good\" + 0.015*\"come\" + 0.013*\"time\" + 0.012*\"food\" + 0.009*\"like\" + 0.008*\"place\" + 0.008*\"try\" + 0.007*\"order\"\n",
      "INFO : topic #47 (0.020): 0.028*\"order\" + 0.018*\"pizza\" + 0.017*\"food\" + 0.012*\"good\" + 0.011*\"like\" + 0.010*\"come\" + 0.009*\"service\" + 0.009*\"place\" + 0.009*\"cheese\" + 0.008*\"ask\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic diff=0.221716, rho=0.161047\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.676 per-word bound, 204.5 perplexity estimate based on a held-out corpus of 2225 documents with 88742 words\n",
      "INFO : PROGRESS: pass 7, dispatched chunk #0 = documents up to #4000/126225, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 7, dispatched chunk #1 = documents up to #8000/126225, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 7, dispatched chunk #2 = documents up to #12000/126225, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 7, dispatched chunk #3 = documents up to #16000/126225, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 7, dispatched chunk #4 = documents up to #20000/126225, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 7, dispatched chunk #5 = documents up to #24000/126225, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 7, dispatched chunk #6 = documents up to #28000/126225, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 7, dispatched chunk #7 = documents up to #32000/126225, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 7, dispatched chunk #8 = documents up to #36000/126225, outstanding queue size 9\n",
      "INFO : PROGRESS: pass 7, dispatched chunk #9 = documents up to #40000/126225, outstanding queue size 10\n",
      "INFO : PROGRESS: pass 7, dispatched chunk #10 = documents up to #44000/126225, outstanding queue size 11\n",
      "INFO : PROGRESS: pass 7, dispatched chunk #11 = documents up to #48000/126225, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 7, dispatched chunk #12 = documents up to #52000/126225, outstanding queue size 13\n",
      "DEBUG : processing chunk #0 of 4000 documents\n",
      "INFO : PROGRESS: pass 7, dispatched chunk #13 = documents up to #56000/126225, outstanding queue size 14\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 7, dispatched chunk #14 = documents up to #60000/126225, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 7, dispatched chunk #15 = documents up to #64000/126225, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 7, dispatched chunk #16 = documents up to #68000/126225, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 7, dispatched chunk #17 = documents up to #72000/126225, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 7, dispatched chunk #18 = documents up to #76000/126225, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 7, dispatched chunk #19 = documents up to #80000/126225, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 7, dispatched chunk #20 = documents up to #84000/126225, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 7, dispatched chunk #21 = documents up to #88000/126225, outstanding queue size 22DEBUG : processing chunk #1 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "\n",
      "INFO : PROGRESS: pass 7, dispatched chunk #22 = documents up to #92000/126225, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 7, dispatched chunk #23 = documents up to #96000/126225, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 7, dispatched chunk #24 = documents up to #100000/126225, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 7, dispatched chunk #25 = documents up to #104000/126225, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 7, dispatched chunk #26 = documents up to #108000/126225, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 7, dispatched chunk #27 = documents up to #112000/126225, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 7, dispatched chunk #28 = documents up to #116000/126225, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 7, dispatched chunk #29 = documents up to #120000/126225, outstanding queue size 30\n",
      "DEBUG : processing chunk #2 of 4000 documents\n",
      "INFO : PROGRESS: pass 7, dispatched chunk #30 = documents up to #124000/126225, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 7, dispatched chunk #31 = documents up to #126225/126225, outstanding queue size 32\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #3 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #4 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #5 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #6 of 4000 documents\n",
      "DEBUG : 3301/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3176/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #7 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 3190/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #8 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3207/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #9 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #10 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #11 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3190/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 3213/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #12 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3193/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #13 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #14 of 4000 documents\n",
      "DEBUG : processing chunk #15 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #16 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #17 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3254/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #18 of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3235/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #19 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3206/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 3209/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #20 of 4000 documents\n",
      "DEBUG : 3217/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #21 of 4000 documents\n",
      "DEBUG : processing chunk #22 of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3237/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #23 of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3297/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #24 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : 3253/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #25 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #26 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3227/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #27 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #28 of 4000 documents\n",
      "DEBUG : 3233/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3243/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #29 of 4000 documents\n",
      "DEBUG : processing chunk #30 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3241/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3231/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #31 of 2225 documents\n",
      "DEBUG : performing inference on a chunk of 2225 documents\n",
      "DEBUG : result put\n",
      "DEBUG : 3221/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3227/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3224/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3204/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3194/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3247/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3207/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 1769/2225 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 3273/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3234/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3180/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 3172/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : topic #36 (0.020): 0.068*\"great\" + 0.052*\"food\" + 0.047*\"good\" + 0.034*\"service\" + 0.032*\"place\" + 0.021*\"love\" + 0.020*\"friendly\" + 0.015*\"staff\" + 0.014*\"definitely\" + 0.013*\"amazing\"\n",
      "INFO : topic #46 (0.020): 0.018*\"place\" + 0.015*\"u\" + 0.012*\"come\" + 0.012*\"game\" + 0.011*\"taco_bell\" + 0.011*\"good\" + 0.008*\"love\" + 0.008*\"price\" + 0.007*\"play\" + 0.007*\"pizza\"\n",
      "INFO : topic #8 (0.020): 0.016*\"time\" + 0.016*\"food\" + 0.015*\"good\" + 0.011*\"service\" + 0.010*\"place\" + 0.009*\"restaurant\" + 0.009*\"favorite\" + 0.009*\"great\" + 0.009*\"come\" + 0.008*\"birthday\"\n",
      "INFO : topic #27 (0.020): 0.035*\"taco\" + 0.031*\"good\" + 0.020*\"food\" + 0.016*\"place\" + 0.013*\"try\" + 0.010*\"order\" + 0.009*\"restaurant\" + 0.009*\"like\" + 0.008*\"mexican\" + 0.008*\"time\"\n",
      "INFO : topic #12 (0.020): 0.022*\"good\" + 0.016*\"come\" + 0.015*\"order\" + 0.015*\"drink\" + 0.010*\"food\" + 0.009*\"place\" + 0.009*\"restaurant\" + 0.008*\"like\" + 0.008*\"little\" + 0.007*\"great\"\n",
      "INFO : topic diff=0.220311, rho=0.158998\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.654 per-word bound, 201.5 perplexity estimate based on a held-out corpus of 2225 documents with 88742 words\n",
      "INFO : PROGRESS: pass 8, dispatched chunk #0 = documents up to #4000/126225, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 8, dispatched chunk #1 = documents up to #8000/126225, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 8, dispatched chunk #2 = documents up to #12000/126225, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 8, dispatched chunk #3 = documents up to #16000/126225, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 8, dispatched chunk #4 = documents up to #20000/126225, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 8, dispatched chunk #5 = documents up to #24000/126225, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 8, dispatched chunk #6 = documents up to #28000/126225, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 8, dispatched chunk #7 = documents up to #32000/126225, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 8, dispatched chunk #8 = documents up to #36000/126225, outstanding queue size 9\n",
      "INFO : PROGRESS: pass 8, dispatched chunk #9 = documents up to #40000/126225, outstanding queue size 10\n",
      "INFO : PROGRESS: pass 8, dispatched chunk #10 = documents up to #44000/126225, outstanding queue size 11\n",
      "INFO : PROGRESS: pass 8, dispatched chunk #11 = documents up to #48000/126225, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 8, dispatched chunk #12 = documents up to #52000/126225, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 8, dispatched chunk #13 = documents up to #56000/126225, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 8, dispatched chunk #14 = documents up to #60000/126225, outstanding queue size 15\n",
      "DEBUG : processing chunk #0 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 8, dispatched chunk #15 = documents up to #64000/126225, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 8, dispatched chunk #16 = documents up to #68000/126225, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 8, dispatched chunk #17 = documents up to #72000/126225, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 8, dispatched chunk #18 = documents up to #76000/126225, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 8, dispatched chunk #19 = documents up to #80000/126225, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 8, dispatched chunk #20 = documents up to #84000/126225, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 8, dispatched chunk #21 = documents up to #88000/126225, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 8, dispatched chunk #22 = documents up to #92000/126225, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 8, dispatched chunk #23 = documents up to #96000/126225, outstanding queue size 24\n",
      "DEBUG : processing chunk #1 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 8, dispatched chunk #24 = documents up to #100000/126225, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 8, dispatched chunk #25 = documents up to #104000/126225, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 8, dispatched chunk #26 = documents up to #108000/126225, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 8, dispatched chunk #27 = documents up to #112000/126225, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 8, dispatched chunk #28 = documents up to #116000/126225, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 8, dispatched chunk #29 = documents up to #120000/126225, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 8, dispatched chunk #30 = documents up to #124000/126225, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 8, dispatched chunk #31 = documents up to #126225/126225, outstanding queue size 32\n",
      "DEBUG : processing chunk #2 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #3 of 4000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #4 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #5 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #6 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #7 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3266/4000 documents converged within 50 iterations\n",
      "DEBUG : 3358/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3256/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #8 of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #9 of 4000 documents\n",
      "DEBUG : processing chunk #10 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #11 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3264/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #12 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3275/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #13 of 4000 documents\n",
      "DEBUG : processing chunk #14 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3272/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #15 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #16 of 4000 documents\n",
      "DEBUG : 3263/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #17 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3315/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #18 of 4000 documents\n",
      "DEBUG : processing chunk #19 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #20 of 4000 documents\n",
      "DEBUG : 3287/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3225/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3266/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #21 of 4000 documents\n",
      "DEBUG : processing chunk #22 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #23 of 4000 documents\n",
      "DEBUG : 3251/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3323/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #24 of 4000 documents\n",
      "DEBUG : 3292/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #25 of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #26 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : 3281/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 3309/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #27 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 3312/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #28 of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3296/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #29 of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #30 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3338/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #31 of 2225 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 2225 documents\n",
      "DEBUG : 3312/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3299/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 3291/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3238/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3262/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3270/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3311/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 1814/2225 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 3255/4000 documents converged within 50 iterations\n",
      "DEBUG : 3247/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3360/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3263/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3183/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : topic #39 (0.020): 0.034*\"food\" + 0.018*\"good\" + 0.015*\"chicken\" + 0.014*\"place\" + 0.013*\"come\" + 0.012*\"rice\" + 0.009*\"eat\" + 0.009*\"great\" + 0.009*\"sauce\" + 0.009*\"order\"\n",
      "INFO : topic #33 (0.020): 0.024*\"breakfast\" + 0.023*\"good\" + 0.018*\"come\" + 0.016*\"food\" + 0.015*\"place\" + 0.015*\"wait\" + 0.011*\"service\" + 0.010*\"great\" + 0.010*\"time\" + 0.008*\"restaurant\"\n",
      "INFO : topic #13 (0.020): 0.028*\"food\" + 0.024*\"good\" + 0.014*\"place\" + 0.012*\"order\" + 0.010*\"come\" + 0.009*\"fresh\" + 0.009*\"time\" + 0.008*\"delicious\" + 0.008*\"like\" + 0.008*\"eat\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #19 (0.020): 0.046*\"burger\" + 0.023*\"place\" + 0.023*\"good\" + 0.017*\"fry\" + 0.014*\"come\" + 0.013*\"like\" + 0.012*\"order\" + 0.009*\"fish\" + 0.009*\"food\" + 0.009*\"try\"\n",
      "INFO : topic #8 (0.020): 0.016*\"time\" + 0.015*\"food\" + 0.015*\"good\" + 0.010*\"service\" + 0.010*\"birthday\" + 0.010*\"place\" + 0.009*\"restaurant\" + 0.009*\"favorite\" + 0.009*\"come\" + 0.009*\"great\"\n",
      "INFO : topic diff=0.218097, rho=0.157026\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.633 per-word bound, 198.5 perplexity estimate based on a held-out corpus of 2225 documents with 88742 words\n",
      "INFO : PROGRESS: pass 9, dispatched chunk #0 = documents up to #4000/126225, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 9, dispatched chunk #1 = documents up to #8000/126225, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 9, dispatched chunk #2 = documents up to #12000/126225, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 9, dispatched chunk #3 = documents up to #16000/126225, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 9, dispatched chunk #4 = documents up to #20000/126225, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 9, dispatched chunk #5 = documents up to #24000/126225, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 9, dispatched chunk #6 = documents up to #28000/126225, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 9, dispatched chunk #7 = documents up to #32000/126225, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 9, dispatched chunk #8 = documents up to #36000/126225, outstanding queue size 9\n",
      "DEBUG : processing chunk #0 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 9, dispatched chunk #9 = documents up to #40000/126225, outstanding queue size 10\n",
      "INFO : PROGRESS: pass 9, dispatched chunk #10 = documents up to #44000/126225, outstanding queue size 11\n",
      "INFO : PROGRESS: pass 9, dispatched chunk #11 = documents up to #48000/126225, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 9, dispatched chunk #12 = documents up to #52000/126225, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 9, dispatched chunk #13 = documents up to #56000/126225, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 9, dispatched chunk #14 = documents up to #60000/126225, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 9, dispatched chunk #15 = documents up to #64000/126225, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 9, dispatched chunk #16 = documents up to #68000/126225, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 9, dispatched chunk #17 = documents up to #72000/126225, outstanding queue size 18\n",
      "DEBUG : processing chunk #1 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 9, dispatched chunk #18 = documents up to #76000/126225, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 9, dispatched chunk #19 = documents up to #80000/126225, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 9, dispatched chunk #20 = documents up to #84000/126225, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 9, dispatched chunk #21 = documents up to #88000/126225, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 9, dispatched chunk #22 = documents up to #92000/126225, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 9, dispatched chunk #23 = documents up to #96000/126225, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 9, dispatched chunk #24 = documents up to #100000/126225, outstanding queue size 25\n",
      "DEBUG : processing chunk #2 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 9, dispatched chunk #25 = documents up to #104000/126225, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 9, dispatched chunk #26 = documents up to #108000/126225, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 9, dispatched chunk #27 = documents up to #112000/126225, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 9, dispatched chunk #28 = documents up to #116000/126225, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 9, dispatched chunk #29 = documents up to #120000/126225, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 9, dispatched chunk #30 = documents up to #124000/126225, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 9, dispatched chunk #31 = documents up to #126225/126225, outstanding queue size 32\n",
      "DEBUG : processing chunk #3 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #4 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #5 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #6 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #7 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #8 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #9 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #10 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3396/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #11 of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3307/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3349/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #12 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #13 of 4000 documents\n",
      "DEBUG : 3300/4000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #14 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #15 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #16 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #17 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3326/4000 documents converged within 50 iterations\n",
      "DEBUG : 3329/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #18 of 4000 documents\n",
      "DEBUG : processing chunk #19 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #20 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3311/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3394/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #21 of 4000 documents\n",
      "DEBUG : processing chunk #22 of 4000 documents\n",
      "DEBUG : 3296/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #23 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #24 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3296/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 3300/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processing chunk #25 of 4000 documents\n",
      "DEBUG : processing chunk #26 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3316/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #27 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #28 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #29 of 4000 documents\n",
      "DEBUG : 3336/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #30 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #31 of 2225 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 2225 documents\n",
      "DEBUG : 3354/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : 3369/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3324/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3346/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3382/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3347/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3329/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 1828/2225 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : 3372/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3334/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : 3294/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3298/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3311/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3341/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3361/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3354/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3304/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 3301/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3273/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : topic #41 (0.020): 0.045*\"order\" + 0.028*\"food\" + 0.024*\"come\" + 0.023*\"time\" + 0.015*\"wait\" + 0.014*\"good\" + 0.011*\"minute\" + 0.010*\"server\" + 0.010*\"service\" + 0.009*\"place\"\n",
      "INFO : topic #3 (0.020): 0.016*\"restaurant\" + 0.013*\"food\" + 0.012*\"dish\" + 0.011*\"good\" + 0.011*\"time\" + 0.010*\"great\" + 0.009*\"service\" + 0.008*\"dinner\" + 0.008*\"come\" + 0.008*\"like\"\n",
      "INFO : topic #30 (0.020): 0.041*\"good\" + 0.020*\"food\" + 0.016*\"steak\" + 0.015*\"service\" + 0.015*\"great\" + 0.013*\"order\" + 0.011*\"come\" + 0.008*\"place\" + 0.008*\"like\" + 0.007*\"try\"\n",
      "INFO : topic #15 (0.020): 0.019*\"wait\" + 0.017*\"table\" + 0.014*\"minute\" + 0.013*\"food\" + 0.012*\"sit\" + 0.012*\"time\" + 0.011*\"service\" + 0.010*\"place\" + 0.010*\"order\" + 0.010*\"come\"\n",
      "INFO : topic #26 (0.020): 0.026*\"food\" + 0.015*\"place\" + 0.013*\"order\" + 0.012*\"come\" + 0.011*\"good\" + 0.011*\"time\" + 0.010*\"room\" + 0.009*\"like\" + 0.008*\"walk\" + 0.008*\"hotel\"\n",
      "INFO : topic diff=0.215212, rho=0.155125\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.616 per-word bound, 196.1 perplexity estimate based on a held-out corpus of 2225 documents with 88742 words\n",
      "INFO : PROGRESS: pass 10, dispatched chunk #0 = documents up to #4000/126225, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 10, dispatched chunk #1 = documents up to #8000/126225, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 10, dispatched chunk #2 = documents up to #12000/126225, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 10, dispatched chunk #3 = documents up to #16000/126225, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 10, dispatched chunk #4 = documents up to #20000/126225, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 10, dispatched chunk #5 = documents up to #24000/126225, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 10, dispatched chunk #6 = documents up to #28000/126225, outstanding queue size 7\n",
      "DEBUG : processing chunk #0 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 10, dispatched chunk #7 = documents up to #32000/126225, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 10, dispatched chunk #8 = documents up to #36000/126225, outstanding queue size 9\n",
      "INFO : PROGRESS: pass 10, dispatched chunk #9 = documents up to #40000/126225, outstanding queue size 10\n",
      "INFO : PROGRESS: pass 10, dispatched chunk #10 = documents up to #44000/126225, outstanding queue size 11\n",
      "INFO : PROGRESS: pass 10, dispatched chunk #11 = documents up to #48000/126225, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 10, dispatched chunk #12 = documents up to #52000/126225, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 10, dispatched chunk #13 = documents up to #56000/126225, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 10, dispatched chunk #14 = documents up to #60000/126225, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 10, dispatched chunk #15 = documents up to #64000/126225, outstanding queue size 16\n",
      "DEBUG : processing chunk #1 of 4000 documents\n",
      "INFO : PROGRESS: pass 10, dispatched chunk #16 = documents up to #68000/126225, outstanding queue size 17\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 10, dispatched chunk #17 = documents up to #72000/126225, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 10, dispatched chunk #18 = documents up to #76000/126225, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 10, dispatched chunk #19 = documents up to #80000/126225, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 10, dispatched chunk #20 = documents up to #84000/126225, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 10, dispatched chunk #21 = documents up to #88000/126225, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 10, dispatched chunk #22 = documents up to #92000/126225, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 10, dispatched chunk #23 = documents up to #96000/126225, outstanding queue size 24\n",
      "DEBUG : processing chunk #2 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 10, dispatched chunk #24 = documents up to #100000/126225, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 10, dispatched chunk #25 = documents up to #104000/126225, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 10, dispatched chunk #26 = documents up to #108000/126225, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 10, dispatched chunk #27 = documents up to #112000/126225, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 10, dispatched chunk #28 = documents up to #116000/126225, outstanding queue size 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 10, dispatched chunk #29 = documents up to #120000/126225, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 10, dispatched chunk #30 = documents up to #124000/126225, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 10, dispatched chunk #31 = documents up to #126225/126225, outstanding queue size 32\n",
      "DEBUG : processing chunk #3 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #4 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #5 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #6 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #7 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3447/4000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #8 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3341/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #9 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #10 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3366/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 3369/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #11 of 4000 documents\n",
      "DEBUG : processing chunk #12 of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3350/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #13 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #14 of 4000 documents\n",
      "DEBUG : 3375/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3371/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #15 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #16 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #17 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3423/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #18 of 4000 documents\n",
      "DEBUG : processing chunk #19 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #20 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3375/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #21 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #22 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3323/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 3340/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 3333/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #23 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3388/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #24 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #25 of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #26 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3413/4000 documents converged within 50 iterations\n",
      "DEBUG : 3380/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #27 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3390/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #28 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #29 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3381/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #30 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3385/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #31 of 2225 documents\n",
      "DEBUG : performing inference on a chunk of 2225 documents\n",
      "DEBUG : 3375/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 3414/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3382/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3348/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3411/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3364/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3321/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 3399/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 1868/2225 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3404/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3378/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3380/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3322/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3294/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : topic #31 (0.020): 0.026*\"buffet\" + 0.020*\"good\" + 0.015*\"food\" + 0.013*\"try\" + 0.013*\"eat\" + 0.011*\"come\" + 0.011*\"place\" + 0.009*\"like\" + 0.009*\"dessert\" + 0.009*\"wait\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #45 (0.020): 0.036*\"food\" + 0.022*\"service\" + 0.020*\"amazing\" + 0.019*\"time\" + 0.017*\"good\" + 0.017*\"place\" + 0.016*\"come\" + 0.012*\"great\" + 0.009*\"wait\" + 0.008*\"restaurant\"\n",
      "INFO : topic #35 (0.020): 0.048*\"order\" + 0.018*\"cheese\" + 0.017*\"chicken\" + 0.017*\"time\" + 0.015*\"sandwich\" + 0.013*\"food\" + 0.012*\"fry\" + 0.012*\"good\" + 0.010*\"come\" + 0.009*\"like\"\n",
      "INFO : topic #23 (0.020): 0.021*\"food\" + 0.018*\"table\" + 0.015*\"order\" + 0.014*\"good\" + 0.012*\"drink\" + 0.012*\"ask\" + 0.012*\"come\" + 0.012*\"seat\" + 0.011*\"place\" + 0.011*\"great\"\n",
      "INFO : topic #8 (0.020): 0.016*\"time\" + 0.015*\"birthday\" + 0.014*\"food\" + 0.014*\"good\" + 0.010*\"service\" + 0.009*\"place\" + 0.009*\"favorite\" + 0.009*\"restaurant\" + 0.009*\"come\" + 0.008*\"great\"\n",
      "INFO : topic diff=0.211772, rho=0.153292\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.599 per-word bound, 193.9 perplexity estimate based on a held-out corpus of 2225 documents with 88742 words\n",
      "INFO : PROGRESS: pass 11, dispatched chunk #0 = documents up to #4000/126225, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 11, dispatched chunk #1 = documents up to #8000/126225, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 11, dispatched chunk #2 = documents up to #12000/126225, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 11, dispatched chunk #3 = documents up to #16000/126225, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 11, dispatched chunk #4 = documents up to #20000/126225, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 11, dispatched chunk #5 = documents up to #24000/126225, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 11, dispatched chunk #6 = documents up to #28000/126225, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 11, dispatched chunk #7 = documents up to #32000/126225, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 11, dispatched chunk #8 = documents up to #36000/126225, outstanding queue size 9\n",
      "INFO : PROGRESS: pass 11, dispatched chunk #9 = documents up to #40000/126225, outstanding queue size 10\n",
      "INFO : PROGRESS: pass 11, dispatched chunk #10 = documents up to #44000/126225, outstanding queue size 11\n",
      "DEBUG : processing chunk #0 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 11, dispatched chunk #11 = documents up to #48000/126225, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 11, dispatched chunk #12 = documents up to #52000/126225, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 11, dispatched chunk #13 = documents up to #56000/126225, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 11, dispatched chunk #14 = documents up to #60000/126225, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 11, dispatched chunk #15 = documents up to #64000/126225, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 11, dispatched chunk #16 = documents up to #68000/126225, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 11, dispatched chunk #17 = documents up to #72000/126225, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 11, dispatched chunk #18 = documents up to #76000/126225, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 11, dispatched chunk #19 = documents up to #80000/126225, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 11, dispatched chunk #20 = documents up to #84000/126225, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 11, dispatched chunk #21 = documents up to #88000/126225, outstanding queue size 22\n",
      "DEBUG : processing chunk #1 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 11, dispatched chunk #22 = documents up to #92000/126225, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 11, dispatched chunk #23 = documents up to #96000/126225, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 11, dispatched chunk #24 = documents up to #100000/126225, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 11, dispatched chunk #25 = documents up to #104000/126225, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 11, dispatched chunk #26 = documents up to #108000/126225, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 11, dispatched chunk #27 = documents up to #112000/126225, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 11, dispatched chunk #28 = documents up to #116000/126225, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 11, dispatched chunk #29 = documents up to #120000/126225, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 11, dispatched chunk #30 = documents up to #124000/126225, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 11, dispatched chunk #31 = documents up to #126225/126225, outstanding queue size 32\n",
      "DEBUG : processing chunk #2 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #3 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #4 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #5 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #6 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #7 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #8 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #9 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3452/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #10 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3429/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #11 of 4000 documents\n",
      "DEBUG : processing chunk #12 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3389/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #13 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #14 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3409/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #15 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3401/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #16 of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3369/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #17 of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3418/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #18 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 3443/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #19 of 4000 documents\n",
      "DEBUG : 3392/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #20 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 3352/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #21 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #22 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #23 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3388/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #24 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3374/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 3419/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #25 of 4000 documents\n",
      "DEBUG : 3447/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #26 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #27 of 4000 documents\n",
      "DEBUG : 3438/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3444/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3449/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #28 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3451/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #29 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3434/4000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #30 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #31 of 2225 documents\n",
      "DEBUG : performing inference on a chunk of 2225 documents\n",
      "DEBUG : 3450/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3375/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 3438/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3398/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3400/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3394/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 1896/2225 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 3423/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3415/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3401/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3413/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3334/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3362/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : topic #22 (0.020): 0.020*\"place\" + 0.015*\"food\" + 0.014*\"good\" + 0.012*\"come\" + 0.010*\"great\" + 0.009*\"vegas\" + 0.009*\"drink\" + 0.009*\"order\" + 0.009*\"service\" + 0.007*\"delicious\"\n",
      "INFO : topic #33 (0.020): 0.032*\"breakfast\" + 0.023*\"good\" + 0.018*\"come\" + 0.015*\"food\" + 0.015*\"place\" + 0.015*\"wait\" + 0.011*\"service\" + 0.010*\"great\" + 0.010*\"time\" + 0.009*\"brunch\"\n",
      "INFO : topic #27 (0.020): 0.043*\"taco\" + 0.031*\"good\" + 0.020*\"food\" + 0.016*\"place\" + 0.013*\"try\" + 0.012*\"mexican\" + 0.010*\"order\" + 0.009*\"like\" + 0.009*\"salsa\" + 0.008*\"restaurant\"\n",
      "INFO : topic #9 (0.020): 0.017*\"great\" + 0.016*\"food\" + 0.013*\"good\" + 0.013*\"come\" + 0.011*\"place\" + 0.009*\"restaurant\" + 0.008*\"amazing\" + 0.007*\"order\" + 0.007*\"fun\" + 0.007*\"service\"\n",
      "INFO : topic #34 (0.020): 0.033*\"vegan\" + 0.017*\"great\" + 0.015*\"good\" + 0.015*\"place\" + 0.011*\"food\" + 0.011*\"time\" + 0.010*\"service\" + 0.008*\"enjoy\" + 0.008*\"delicious\" + 0.008*\"meal\"\n",
      "INFO : topic diff=0.207657, rho=0.151522\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.585 per-word bound, 192.0 perplexity estimate based on a held-out corpus of 2225 documents with 88742 words\n",
      "INFO : PROGRESS: pass 12, dispatched chunk #0 = documents up to #4000/126225, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 12, dispatched chunk #1 = documents up to #8000/126225, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 12, dispatched chunk #2 = documents up to #12000/126225, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 12, dispatched chunk #3 = documents up to #16000/126225, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 12, dispatched chunk #4 = documents up to #20000/126225, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 12, dispatched chunk #5 = documents up to #24000/126225, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 12, dispatched chunk #6 = documents up to #28000/126225, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 12, dispatched chunk #7 = documents up to #32000/126225, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 12, dispatched chunk #8 = documents up to #36000/126225, outstanding queue size 9\n",
      "INFO : PROGRESS: pass 12, dispatched chunk #9 = documents up to #40000/126225, outstanding queue size 10\n",
      "INFO : PROGRESS: pass 12, dispatched chunk #10 = documents up to #44000/126225, outstanding queue size 11\n",
      "INFO : PROGRESS: pass 12, dispatched chunk #11 = documents up to #48000/126225, outstanding queue size 12DEBUG : processing chunk #0 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "\n",
      "INFO : PROGRESS: pass 12, dispatched chunk #12 = documents up to #52000/126225, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 12, dispatched chunk #13 = documents up to #56000/126225, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 12, dispatched chunk #14 = documents up to #60000/126225, outstanding queue size 15DEBUG : processing chunk #1 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "\n",
      "INFO : PROGRESS: pass 12, dispatched chunk #15 = documents up to #64000/126225, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 12, dispatched chunk #16 = documents up to #68000/126225, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 12, dispatched chunk #17 = documents up to #72000/126225, outstanding queue size 18\n",
      "DEBUG : processing chunk #2 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 12, dispatched chunk #18 = documents up to #76000/126225, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 12, dispatched chunk #19 = documents up to #80000/126225, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 12, dispatched chunk #20 = documents up to #84000/126225, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 12, dispatched chunk #21 = documents up to #88000/126225, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 12, dispatched chunk #22 = documents up to #92000/126225, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 12, dispatched chunk #23 = documents up to #96000/126225, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 12, dispatched chunk #24 = documents up to #100000/126225, outstanding queue size 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 12, dispatched chunk #25 = documents up to #104000/126225, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 12, dispatched chunk #26 = documents up to #108000/126225, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 12, dispatched chunk #27 = documents up to #112000/126225, outstanding queue size 28DEBUG : processing chunk #3 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "\n",
      "INFO : PROGRESS: pass 12, dispatched chunk #28 = documents up to #116000/126225, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 12, dispatched chunk #29 = documents up to #120000/126225, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 12, dispatched chunk #30 = documents up to #124000/126225, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 12, dispatched chunk #31 = documents up to #126225/126225, outstanding queue size 32\n",
      "DEBUG : processing chunk #4 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #5 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #6 of 4000 documents\n",
      "DEBUG : processing chunk #7 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3472/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #8 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3422/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #9 of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3442/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #10 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3432/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #11 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3401/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #12 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3437/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #13 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3423/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #14 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #15 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #16 of 4000 documents\n",
      "DEBUG : 3453/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #17 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 3429/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #18 of 4000 documents\n",
      "DEBUG : processing chunk #19 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3412/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #20 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3420/4000 documents converged within 50 iterations\n",
      "DEBUG : 3465/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #21 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #22 of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3425/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #23 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3474/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #24 of 4000 documents\n",
      "DEBUG : processing chunk #25 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #26 of 4000 documents\n",
      "DEBUG : 3460/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #27 of 4000 documents\n",
      "DEBUG : 3455/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #28 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3458/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3465/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #29 of 4000 documents\n",
      "DEBUG : processing chunk #30 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 3437/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #31 of 2225 documents\n",
      "DEBUG : performing inference on a chunk of 2225 documents\n",
      "DEBUG : 3441/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3467/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 3421/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3429/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3427/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : 3398/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3425/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3486/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 1913/2225 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3461/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3393/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : getting a new job\n",
      "DEBUG : 3341/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3438/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : topic #34 (0.020): 0.036*\"vegan\" + 0.017*\"great\" + 0.015*\"place\" + 0.015*\"good\" + 0.011*\"food\" + 0.011*\"time\" + 0.010*\"service\" + 0.008*\"enjoy\" + 0.008*\"delicious\" + 0.008*\"meal\"\n",
      "INFO : topic #42 (0.020): 0.029*\"order\" + 0.023*\"ask\" + 0.023*\"tell\" + 0.019*\"manager\" + 0.013*\"come\" + 0.013*\"food\" + 0.011*\"want\" + 0.010*\"bad\" + 0.009*\"like\" + 0.009*\"charge\"\n",
      "INFO : topic #23 (0.020): 0.021*\"food\" + 0.019*\"table\" + 0.014*\"order\" + 0.014*\"good\" + 0.014*\"drink\" + 0.014*\"seat\" + 0.012*\"ask\" + 0.012*\"come\" + 0.011*\"place\" + 0.010*\"great\"\n",
      "INFO : topic #24 (0.020): 0.044*\"good\" + 0.032*\"food\" + 0.017*\"place\" + 0.014*\"time\" + 0.013*\"come\" + 0.010*\"eat\" + 0.008*\"great\" + 0.008*\"love\" + 0.008*\"order\" + 0.008*\"try\"\n",
      "INFO : topic #39 (0.020): 0.032*\"food\" + 0.019*\"good\" + 0.018*\"chicken\" + 0.017*\"rice\" + 0.014*\"place\" + 0.012*\"come\" + 0.010*\"sauce\" + 0.009*\"order\" + 0.009*\"eat\" + 0.008*\"try\"\n",
      "INFO : topic diff=0.203198, rho=0.149812\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.573 per-word bound, 190.4 perplexity estimate based on a held-out corpus of 2225 documents with 88742 words\n",
      "INFO : PROGRESS: pass 13, dispatched chunk #0 = documents up to #4000/126225, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 13, dispatched chunk #1 = documents up to #8000/126225, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 13, dispatched chunk #2 = documents up to #12000/126225, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 13, dispatched chunk #3 = documents up to #16000/126225, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 13, dispatched chunk #4 = documents up to #20000/126225, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 13, dispatched chunk #5 = documents up to #24000/126225, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 13, dispatched chunk #6 = documents up to #28000/126225, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 13, dispatched chunk #7 = documents up to #32000/126225, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 13, dispatched chunk #8 = documents up to #36000/126225, outstanding queue size 9\n",
      "INFO : PROGRESS: pass 13, dispatched chunk #9 = documents up to #40000/126225, outstanding queue size 10\n",
      "INFO : PROGRESS: pass 13, dispatched chunk #10 = documents up to #44000/126225, outstanding queue size 11\n",
      "INFO : PROGRESS: pass 13, dispatched chunk #11 = documents up to #48000/126225, outstanding queue size 12\n",
      "DEBUG : processing chunk #0 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 13, dispatched chunk #12 = documents up to #52000/126225, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 13, dispatched chunk #13 = documents up to #56000/126225, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 13, dispatched chunk #14 = documents up to #60000/126225, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 13, dispatched chunk #15 = documents up to #64000/126225, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 13, dispatched chunk #16 = documents up to #68000/126225, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 13, dispatched chunk #17 = documents up to #72000/126225, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 13, dispatched chunk #18 = documents up to #76000/126225, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 13, dispatched chunk #19 = documents up to #80000/126225, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 13, dispatched chunk #20 = documents up to #84000/126225, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 13, dispatched chunk #21 = documents up to #88000/126225, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 13, dispatched chunk #22 = documents up to #92000/126225, outstanding queue size 23DEBUG : processing chunk #1 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "\n",
      "INFO : PROGRESS: pass 13, dispatched chunk #23 = documents up to #96000/126225, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 13, dispatched chunk #24 = documents up to #100000/126225, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 13, dispatched chunk #25 = documents up to #104000/126225, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 13, dispatched chunk #26 = documents up to #108000/126225, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 13, dispatched chunk #27 = documents up to #112000/126225, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 13, dispatched chunk #28 = documents up to #116000/126225, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 13, dispatched chunk #29 = documents up to #120000/126225, outstanding queue size 30\n",
      "DEBUG : processing chunk #2 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 13, dispatched chunk #30 = documents up to #124000/126225, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 13, dispatched chunk #31 = documents up to #126225/126225, outstanding queue size 32\n",
      "DEBUG : processing chunk #3 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #5 of 4000 documents\n",
      "DEBUG : processing chunk #4 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #6 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #7 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #8 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3527/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 3450/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #9 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 3475/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #10 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #11 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3457/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #12 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #13 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 3441/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #14 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #15 of 4000 documents\n",
      "DEBUG : 3461/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3458/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #16 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3479/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #17 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #18 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #19 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3471/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #20 of 4000 documents\n",
      "DEBUG : processing chunk #21 of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3445/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #22 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #23 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3460/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 3466/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #24 of 4000 documents\n",
      "DEBUG : 3452/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #25 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #26 of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3471/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : 3456/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #27 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #28 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3464/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #29 of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3444/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #30 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3472/4000 documents converged within 50 iterations\n",
      "DEBUG : 3503/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #31 of 2225 documents\n",
      "DEBUG : performing inference on a chunk of 2225 documents\n",
      "DEBUG : 3471/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3438/4000 documents converged within 50 iterations\n",
      "DEBUG : 3503/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3517/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 3415/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3462/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 3463/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3499/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 1917/2225 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 3434/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3467/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3442/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3400/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : topic #3 (0.020): 0.017*\"restaurant\" + 0.014*\"dish\" + 0.012*\"food\" + 0.010*\"good\" + 0.009*\"time\" + 0.009*\"dinner\" + 0.009*\"great\" + 0.008*\"service\" + 0.008*\"menu\" + 0.008*\"come\"\n",
      "INFO : topic #21 (0.020): 0.032*\"steak\" + 0.013*\"good\" + 0.012*\"chocolate\" + 0.009*\"like\" + 0.009*\"time\" + 0.008*\"dessert\" + 0.008*\"order\" + 0.007*\"try\" + 0.007*\"eat\" + 0.006*\"prime_rib\"\n",
      "INFO : topic #2 (0.020): 0.025*\"food\" + 0.018*\"like\" + 0.015*\"good\" + 0.010*\"come\" + 0.010*\"great\" + 0.009*\"time\" + 0.007*\"service\" + 0.007*\"place\" + 0.007*\"eat\" + 0.007*\"order\"\n",
      "INFO : topic #45 (0.020): 0.037*\"food\" + 0.023*\"service\" + 0.022*\"amazing\" + 0.020*\"time\" + 0.018*\"good\" + 0.017*\"place\" + 0.017*\"come\" + 0.011*\"great\" + 0.009*\"wait\" + 0.009*\"restaurant\"\n",
      "INFO : topic #36 (0.020): 0.071*\"great\" + 0.059*\"food\" + 0.046*\"good\" + 0.036*\"place\" + 0.036*\"service\" + 0.022*\"love\" + 0.021*\"friendly\" + 0.016*\"amazing\" + 0.016*\"staff\" + 0.016*\"definitely\"\n",
      "INFO : topic diff=0.198516, rho=0.148158\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.561 per-word bound, 188.9 perplexity estimate based on a held-out corpus of 2225 documents with 88742 words\n",
      "INFO : PROGRESS: pass 14, dispatched chunk #0 = documents up to #4000/126225, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 14, dispatched chunk #1 = documents up to #8000/126225, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 14, dispatched chunk #2 = documents up to #12000/126225, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 14, dispatched chunk #3 = documents up to #16000/126225, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 14, dispatched chunk #4 = documents up to #20000/126225, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 14, dispatched chunk #5 = documents up to #24000/126225, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 14, dispatched chunk #6 = documents up to #28000/126225, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 14, dispatched chunk #7 = documents up to #32000/126225, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 14, dispatched chunk #8 = documents up to #36000/126225, outstanding queue size 9\n",
      "DEBUG : processing chunk #0 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 14, dispatched chunk #9 = documents up to #40000/126225, outstanding queue size 10\n",
      "INFO : PROGRESS: pass 14, dispatched chunk #10 = documents up to #44000/126225, outstanding queue size 11\n",
      "INFO : PROGRESS: pass 14, dispatched chunk #11 = documents up to #48000/126225, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 14, dispatched chunk #12 = documents up to #52000/126225, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 14, dispatched chunk #13 = documents up to #56000/126225, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 14, dispatched chunk #14 = documents up to #60000/126225, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 14, dispatched chunk #15 = documents up to #64000/126225, outstanding queue size 16\n",
      "DEBUG : processing chunk #1 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 14, dispatched chunk #16 = documents up to #68000/126225, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 14, dispatched chunk #17 = documents up to #72000/126225, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 14, dispatched chunk #18 = documents up to #76000/126225, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 14, dispatched chunk #19 = documents up to #80000/126225, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 14, dispatched chunk #20 = documents up to #84000/126225, outstanding queue size 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 14, dispatched chunk #21 = documents up to #88000/126225, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 14, dispatched chunk #22 = documents up to #92000/126225, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 14, dispatched chunk #23 = documents up to #96000/126225, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 14, dispatched chunk #24 = documents up to #100000/126225, outstanding queue size 25DEBUG : processing chunk #2 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "\n",
      "INFO : PROGRESS: pass 14, dispatched chunk #25 = documents up to #104000/126225, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 14, dispatched chunk #26 = documents up to #108000/126225, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 14, dispatched chunk #27 = documents up to #112000/126225, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 14, dispatched chunk #28 = documents up to #116000/126225, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 14, dispatched chunk #29 = documents up to #120000/126225, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 14, dispatched chunk #30 = documents up to #124000/126225, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 14, dispatched chunk #31 = documents up to #126225/126225, outstanding queue size 32\n",
      "DEBUG : processing chunk #3 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #4 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #5 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #6 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #7 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #8 of 4000 documents\n",
      "DEBUG : 3466/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3503/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3454/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #9 of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3494/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #10 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 3486/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #11 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3468/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #12 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : 3509/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #13 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3473/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #14 of 4000 documents\n",
      "DEBUG : processing chunk #15 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3525/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #16 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #17 of 4000 documents\n",
      "DEBUG : 3492/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #18 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #19 of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3500/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #20 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #21 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : 3464/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #22 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3439/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #23 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #24 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #25 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3478/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #26 of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3501/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3506/4000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #27 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #28 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3487/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #29 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : 3502/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #30 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #31 of 2225 documents\n",
      "DEBUG : performing inference on a chunk of 2225 documents\n",
      "DEBUG : 3500/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3457/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 3503/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3472/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3506/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : 3442/4000 documents converged within 50 iterations\n",
      "DEBUG : 3476/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 1916/2225 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3494/4000 documents converged within 50 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3522/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3450/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 3488/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3422/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : 3435/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : topic #19 (0.020): 0.061*\"burger\" + 0.025*\"good\" + 0.023*\"fry\" + 0.021*\"place\" + 0.013*\"like\" + 0.013*\"come\" + 0.012*\"order\" + 0.010*\"fish\" + 0.010*\"try\" + 0.008*\"food\"\n",
      "INFO : topic #9 (0.020): 0.017*\"great\" + 0.016*\"food\" + 0.013*\"come\" + 0.012*\"good\" + 0.011*\"place\" + 0.009*\"restaurant\" + 0.008*\"fun\" + 0.008*\"amazing\" + 0.007*\"order\" + 0.006*\"like\"\n",
      "INFO : topic #13 (0.020): 0.027*\"food\" + 0.023*\"good\" + 0.014*\"place\" + 0.011*\"order\" + 0.010*\"fresh\" + 0.009*\"come\" + 0.008*\"like\" + 0.008*\"delicious\" + 0.008*\"eat\" + 0.008*\"time\"\n",
      "INFO : topic #22 (0.020): 0.020*\"place\" + 0.014*\"food\" + 0.014*\"good\" + 0.012*\"come\" + 0.010*\"vegas\" + 0.009*\"great\" + 0.009*\"drink\" + 0.008*\"order\" + 0.008*\"service\" + 0.007*\"delicious\"\n",
      "INFO : topic #1 (0.020): 0.024*\"bar\" + 0.017*\"crepe\" + 0.015*\"good\" + 0.013*\"place\" + 0.010*\"drink\" + 0.010*\"nice\" + 0.009*\"food\" + 0.009*\"area\" + 0.008*\"great\" + 0.007*\"service\"\n",
      "INFO : topic diff=0.193409, rho=0.146558\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.551 per-word bound, 187.5 perplexity estimate based on a held-out corpus of 2225 documents with 88742 words\n",
      "INFO : PROGRESS: pass 15, dispatched chunk #0 = documents up to #4000/126225, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 15, dispatched chunk #1 = documents up to #8000/126225, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 15, dispatched chunk #2 = documents up to #12000/126225, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 15, dispatched chunk #3 = documents up to #16000/126225, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 15, dispatched chunk #4 = documents up to #20000/126225, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 15, dispatched chunk #5 = documents up to #24000/126225, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 15, dispatched chunk #6 = documents up to #28000/126225, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 15, dispatched chunk #7 = documents up to #32000/126225, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 15, dispatched chunk #8 = documents up to #36000/126225, outstanding queue size 9\n",
      "DEBUG : processing chunk #0 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 15, dispatched chunk #9 = documents up to #40000/126225, outstanding queue size 10\n",
      "INFO : PROGRESS: pass 15, dispatched chunk #10 = documents up to #44000/126225, outstanding queue size 11\n",
      "INFO : PROGRESS: pass 15, dispatched chunk #11 = documents up to #48000/126225, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 15, dispatched chunk #12 = documents up to #52000/126225, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 15, dispatched chunk #13 = documents up to #56000/126225, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 15, dispatched chunk #14 = documents up to #60000/126225, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 15, dispatched chunk #15 = documents up to #64000/126225, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 15, dispatched chunk #16 = documents up to #68000/126225, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 15, dispatched chunk #17 = documents up to #72000/126225, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 15, dispatched chunk #18 = documents up to #76000/126225, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 15, dispatched chunk #19 = documents up to #80000/126225, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 15, dispatched chunk #20 = documents up to #84000/126225, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 15, dispatched chunk #21 = documents up to #88000/126225, outstanding queue size 22DEBUG : processing chunk #1 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "\n",
      "INFO : PROGRESS: pass 15, dispatched chunk #22 = documents up to #92000/126225, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 15, dispatched chunk #23 = documents up to #96000/126225, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 15, dispatched chunk #24 = documents up to #100000/126225, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 15, dispatched chunk #25 = documents up to #104000/126225, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 15, dispatched chunk #26 = documents up to #108000/126225, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 15, dispatched chunk #27 = documents up to #112000/126225, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 15, dispatched chunk #28 = documents up to #116000/126225, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 15, dispatched chunk #29 = documents up to #120000/126225, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 15, dispatched chunk #30 = documents up to #124000/126225, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 15, dispatched chunk #31 = documents up to #126225/126225, outstanding queue size 32\n",
      "DEBUG : processing chunk #2 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #3 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #4 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #5 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #6 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #7 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3561/4000 documents converged within 50 iterations\n",
      "DEBUG : 3490/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #8 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #9 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3495/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #10 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3516/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #11 of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #12 of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3479/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #13 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3518/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 3522/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #14 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #15 of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3492/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #16 of 4000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #17 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3499/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #18 of 4000 documents\n",
      "DEBUG : 3502/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #19 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3506/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #20 of 4000 documents\n",
      "DEBUG : processing chunk #21 of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3501/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #22 of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3491/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #23 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3534/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #24 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #25 of 4000 documents\n",
      "DEBUG : 3519/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3518/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #26 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3519/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 3518/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #27 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3525/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #28 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #29 of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3530/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #30 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #31 of 2225 documents\n",
      "DEBUG : performing inference on a chunk of 2225 documents\n",
      "DEBUG : 3501/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3506/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3487/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3513/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3485/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 1923/2225 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3488/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3515/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3517/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3479/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3461/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3475/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : topic #24 (0.020): 0.045*\"good\" + 0.032*\"food\" + 0.017*\"place\" + 0.014*\"time\" + 0.013*\"come\" + 0.010*\"eat\" + 0.008*\"love\" + 0.008*\"try\" + 0.008*\"great\" + 0.008*\"order\"\n",
      "INFO : topic #14 (0.020): 0.021*\"good\" + 0.015*\"oyster\" + 0.014*\"order\" + 0.010*\"eat\" + 0.010*\"beer\" + 0.009*\"bar\" + 0.008*\"time\" + 0.008*\"great\" + 0.008*\"try\" + 0.007*\"place\"\n",
      "INFO : topic #11 (0.020): 0.039*\"come\" + 0.039*\"food\" + 0.025*\"eat\" + 0.021*\"time\" + 0.015*\"place\" + 0.013*\"good\" + 0.013*\"bad\" + 0.011*\"pancake\" + 0.011*\"service\" + 0.010*\"try\"\n",
      "INFO : topic #1 (0.020): 0.025*\"bar\" + 0.017*\"crepe\" + 0.014*\"good\" + 0.013*\"place\" + 0.010*\"drink\" + 0.010*\"nice\" + 0.009*\"area\" + 0.009*\"food\" + 0.008*\"great\" + 0.007*\"service\"\n",
      "INFO : topic #3 (0.020): 0.018*\"restaurant\" + 0.014*\"dish\" + 0.012*\"food\" + 0.010*\"good\" + 0.009*\"dinner\" + 0.009*\"time\" + 0.008*\"great\" + 0.008*\"menu\" + 0.008*\"service\" + 0.008*\"experience\"\n",
      "INFO : topic diff=0.188069, rho=0.145009\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.542 per-word bound, 186.3 perplexity estimate based on a held-out corpus of 2225 documents with 88742 words\n",
      "INFO : PROGRESS: pass 16, dispatched chunk #0 = documents up to #4000/126225, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 16, dispatched chunk #1 = documents up to #8000/126225, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 16, dispatched chunk #2 = documents up to #12000/126225, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 16, dispatched chunk #3 = documents up to #16000/126225, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 16, dispatched chunk #4 = documents up to #20000/126225, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 16, dispatched chunk #5 = documents up to #24000/126225, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 16, dispatched chunk #6 = documents up to #28000/126225, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 16, dispatched chunk #7 = documents up to #32000/126225, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 16, dispatched chunk #8 = documents up to #36000/126225, outstanding queue size 9\n",
      "INFO : PROGRESS: pass 16, dispatched chunk #9 = documents up to #40000/126225, outstanding queue size 10\n",
      "DEBUG : processing chunk #0 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 16, dispatched chunk #10 = documents up to #44000/126225, outstanding queue size 11\n",
      "INFO : PROGRESS: pass 16, dispatched chunk #11 = documents up to #48000/126225, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 16, dispatched chunk #12 = documents up to #52000/126225, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 16, dispatched chunk #13 = documents up to #56000/126225, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 16, dispatched chunk #14 = documents up to #60000/126225, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 16, dispatched chunk #15 = documents up to #64000/126225, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 16, dispatched chunk #16 = documents up to #68000/126225, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 16, dispatched chunk #17 = documents up to #72000/126225, outstanding queue size 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processing chunk #1 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 16, dispatched chunk #18 = documents up to #76000/126225, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 16, dispatched chunk #19 = documents up to #80000/126225, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 16, dispatched chunk #20 = documents up to #84000/126225, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 16, dispatched chunk #21 = documents up to #88000/126225, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 16, dispatched chunk #22 = documents up to #92000/126225, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 16, dispatched chunk #23 = documents up to #96000/126225, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 16, dispatched chunk #24 = documents up to #100000/126225, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 16, dispatched chunk #25 = documents up to #104000/126225, outstanding queue size 26\n",
      "DEBUG : processing chunk #2 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 16, dispatched chunk #26 = documents up to #108000/126225, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 16, dispatched chunk #27 = documents up to #112000/126225, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 16, dispatched chunk #28 = documents up to #116000/126225, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 16, dispatched chunk #29 = documents up to #120000/126225, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 16, dispatched chunk #30 = documents up to #124000/126225, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 16, dispatched chunk #31 = documents up to #126225/126225, outstanding queue size 32\n",
      "DEBUG : processing chunk #3 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #4 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #5 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #6 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #7 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3558/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #8 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3512/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 3501/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3532/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #9 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #10 of 4000 documents\n",
      "DEBUG : 3505/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #11 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #12 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3514/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #13 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3512/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #14 of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3566/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3532/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #15 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #16 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #17 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3525/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 3531/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #18 of 4000 documents\n",
      "DEBUG : processing chunk #19 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3496/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #20 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #21 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3512/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #22 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #23 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #24 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3518/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 3571/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #25 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3539/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #26 of 4000 documents\n",
      "DEBUG : processing chunk #27 of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3514/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3533/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3546/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #28 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #29 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #30 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3543/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #31 of 2225 documents\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 2225 documents\n",
      "DEBUG : 3529/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3524/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3497/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3542/4000 documents converged within 50 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3486/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3541/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 1943/2225 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3519/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3523/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3487/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3475/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3448/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : topic #28 (0.020): 0.022*\"place\" + 0.015*\"food\" + 0.014*\"like\" + 0.012*\"good\" + 0.012*\"hot\" + 0.011*\"eat\" + 0.011*\"order\" + 0.010*\"try\" + 0.009*\"come\" + 0.009*\"love\"\n",
      "INFO : topic #35 (0.020): 0.050*\"order\" + 0.020*\"sandwich\" + 0.019*\"cheese\" + 0.019*\"chicken\" + 0.017*\"time\" + 0.013*\"food\" + 0.013*\"fry\" + 0.011*\"good\" + 0.010*\"like\" + 0.010*\"come\"\n",
      "INFO : topic #37 (0.020): 0.037*\"salad\" + 0.033*\"good\" + 0.023*\"pasta\" + 0.018*\"italian\" + 0.014*\"restaurant\" + 0.013*\"sauce\" + 0.013*\"food\" + 0.010*\"fresh\" + 0.010*\"try\" + 0.010*\"delicious\"\n",
      "INFO : topic #44 (0.020): 0.061*\"chicken\" + 0.031*\"fry\" + 0.026*\"waffle\" + 0.025*\"good\" + 0.015*\"come\" + 0.014*\"order\" + 0.011*\"food\" + 0.008*\"sauce\" + 0.008*\"crispy\" + 0.008*\"delicious\"\n",
      "INFO : topic #3 (0.020): 0.018*\"restaurant\" + 0.015*\"dish\" + 0.011*\"food\" + 0.010*\"good\" + 0.010*\"dinner\" + 0.009*\"time\" + 0.008*\"menu\" + 0.008*\"great\" + 0.008*\"service\" + 0.008*\"experience\"\n",
      "INFO : topic diff=0.182489, rho=0.143508\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.533 per-word bound, 185.2 perplexity estimate based on a held-out corpus of 2225 documents with 88742 words\n",
      "INFO : PROGRESS: pass 17, dispatched chunk #0 = documents up to #4000/126225, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 17, dispatched chunk #1 = documents up to #8000/126225, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 17, dispatched chunk #2 = documents up to #12000/126225, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 17, dispatched chunk #3 = documents up to #16000/126225, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 17, dispatched chunk #4 = documents up to #20000/126225, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 17, dispatched chunk #5 = documents up to #24000/126225, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 17, dispatched chunk #6 = documents up to #28000/126225, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 17, dispatched chunk #7 = documents up to #32000/126225, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 17, dispatched chunk #8 = documents up to #36000/126225, outstanding queue size 9\n",
      "DEBUG : processing chunk #0 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 17, dispatched chunk #9 = documents up to #40000/126225, outstanding queue size 10\n",
      "INFO : PROGRESS: pass 17, dispatched chunk #10 = documents up to #44000/126225, outstanding queue size 11\n",
      "INFO : PROGRESS: pass 17, dispatched chunk #11 = documents up to #48000/126225, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 17, dispatched chunk #12 = documents up to #52000/126225, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 17, dispatched chunk #13 = documents up to #56000/126225, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 17, dispatched chunk #14 = documents up to #60000/126225, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 17, dispatched chunk #15 = documents up to #64000/126225, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 17, dispatched chunk #16 = documents up to #68000/126225, outstanding queue size 17\n",
      "DEBUG : processing chunk #1 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 17, dispatched chunk #17 = documents up to #72000/126225, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 17, dispatched chunk #18 = documents up to #76000/126225, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 17, dispatched chunk #19 = documents up to #80000/126225, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 17, dispatched chunk #20 = documents up to #84000/126225, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 17, dispatched chunk #21 = documents up to #88000/126225, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 17, dispatched chunk #22 = documents up to #92000/126225, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 17, dispatched chunk #23 = documents up to #96000/126225, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 17, dispatched chunk #24 = documents up to #100000/126225, outstanding queue size 25\n",
      "DEBUG : processing chunk #2 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 17, dispatched chunk #25 = documents up to #104000/126225, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 17, dispatched chunk #26 = documents up to #108000/126225, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 17, dispatched chunk #27 = documents up to #112000/126225, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 17, dispatched chunk #28 = documents up to #116000/126225, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 17, dispatched chunk #29 = documents up to #120000/126225, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 17, dispatched chunk #30 = documents up to #124000/126225, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 17, dispatched chunk #31 = documents up to #126225/126225, outstanding queue size 32\n",
      "DEBUG : processing chunk #3 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #4 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #5 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #6 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #7 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #8 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3575/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #9 of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3504/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3547/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #10 of 4000 documents\n",
      "DEBUG : 3553/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #11 of 4000 documents\n",
      "DEBUG : processing chunk #12 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3508/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 3505/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #13 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #14 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : 3554/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #15 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3563/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #16 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3564/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #17 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3525/4000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #18 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #19 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3522/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3521/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #20 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #21 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3561/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #22 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3536/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #23 of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : 3540/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #24 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3550/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #25 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #26 of 4000 documents\n",
      "DEBUG : 3548/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #27 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #28 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3562/4000 documents converged within 50 iterations\n",
      "DEBUG : 3545/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #29 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3543/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #30 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #31 of 2225 documents\n",
      "DEBUG : performing inference on a chunk of 2225 documents\n",
      "DEBUG : 3548/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3531/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3509/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : 3532/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3542/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3514/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 1961/2225 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3504/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3547/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3539/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3491/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3484/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : topic #39 (0.020): 0.030*\"food\" + 0.021*\"rice\" + 0.021*\"chicken\" + 0.019*\"good\" + 0.014*\"place\" + 0.012*\"come\" + 0.012*\"sauce\" + 0.010*\"order\" + 0.009*\"like\" + 0.009*\"eat\"\n",
      "INFO : topic #22 (0.020): 0.020*\"place\" + 0.014*\"food\" + 0.013*\"good\" + 0.011*\"come\" + 0.011*\"vegas\" + 0.009*\"drink\" + 0.009*\"great\" + 0.008*\"order\" + 0.008*\"service\" + 0.007*\"delicious\"\n",
      "INFO : topic #20 (0.020): 0.046*\"dog\" + 0.034*\"groupon\" + 0.016*\"good\" + 0.010*\"order\" + 0.010*\"food\" + 0.009*\"waited\" + 0.008*\"time\" + 0.006*\"try\" + 0.006*\"restaurant\" + 0.005*\"eat\"\n",
      "INFO : topic #23 (0.020): 0.023*\"table\" + 0.020*\"food\" + 0.019*\"seat\" + 0.018*\"drink\" + 0.014*\"good\" + 0.014*\"order\" + 0.013*\"ask\" + 0.013*\"come\" + 0.011*\"waitress\" + 0.011*\"place\"\n",
      "INFO : topic #13 (0.020): 0.026*\"food\" + 0.023*\"good\" + 0.014*\"place\" + 0.010*\"order\" + 0.010*\"fresh\" + 0.009*\"like\" + 0.008*\"delicious\" + 0.008*\"come\" + 0.008*\"eat\" + 0.008*\"location\"\n",
      "INFO : topic diff=0.176542, rho=0.142053\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.525 per-word bound, 184.2 perplexity estimate based on a held-out corpus of 2225 documents with 88742 words\n",
      "INFO : PROGRESS: pass 18, dispatched chunk #0 = documents up to #4000/126225, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 18, dispatched chunk #1 = documents up to #8000/126225, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 18, dispatched chunk #2 = documents up to #12000/126225, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 18, dispatched chunk #3 = documents up to #16000/126225, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 18, dispatched chunk #4 = documents up to #20000/126225, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 18, dispatched chunk #5 = documents up to #24000/126225, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 18, dispatched chunk #6 = documents up to #28000/126225, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 18, dispatched chunk #7 = documents up to #32000/126225, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 18, dispatched chunk #8 = documents up to #36000/126225, outstanding queue size 9\n",
      "DEBUG : processing chunk #0 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 18, dispatched chunk #9 = documents up to #40000/126225, outstanding queue size 10\n",
      "INFO : PROGRESS: pass 18, dispatched chunk #10 = documents up to #44000/126225, outstanding queue size 11\n",
      "INFO : PROGRESS: pass 18, dispatched chunk #11 = documents up to #48000/126225, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 18, dispatched chunk #12 = documents up to #52000/126225, outstanding queue size 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 18, dispatched chunk #13 = documents up to #56000/126225, outstanding queue size 14\n",
      "DEBUG : processing chunk #1 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 18, dispatched chunk #14 = documents up to #60000/126225, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 18, dispatched chunk #15 = documents up to #64000/126225, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 18, dispatched chunk #16 = documents up to #68000/126225, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 18, dispatched chunk #17 = documents up to #72000/126225, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 18, dispatched chunk #18 = documents up to #76000/126225, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 18, dispatched chunk #19 = documents up to #80000/126225, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 18, dispatched chunk #20 = documents up to #84000/126225, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 18, dispatched chunk #21 = documents up to #88000/126225, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 18, dispatched chunk #22 = documents up to #92000/126225, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 18, dispatched chunk #23 = documents up to #96000/126225, outstanding queue size 24\n",
      "DEBUG : processing chunk #2 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 18, dispatched chunk #24 = documents up to #100000/126225, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 18, dispatched chunk #25 = documents up to #104000/126225, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 18, dispatched chunk #26 = documents up to #108000/126225, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 18, dispatched chunk #27 = documents up to #112000/126225, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 18, dispatched chunk #28 = documents up to #116000/126225, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 18, dispatched chunk #29 = documents up to #120000/126225, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 18, dispatched chunk #30 = documents up to #124000/126225, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 18, dispatched chunk #31 = documents up to #126225/126225, outstanding queue size 32\n",
      "DEBUG : processing chunk #3 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #4 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #5 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #6 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #7 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3611/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 3536/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #8 of 4000 documents\n",
      "DEBUG : 3525/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #9 of 4000 documents\n",
      "DEBUG : 3579/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #10 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3552/4000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #11 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #12 of 4000 documents\n",
      "DEBUG : 3513/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #13 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3541/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #14 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3539/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #15 of 4000 documents\n",
      "DEBUG : processing chunk #16 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #17 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3549/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #18 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3547/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #19 of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3542/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #20 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #21 of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3560/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3547/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #22 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : 3595/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #23 of 4000 documents\n",
      "DEBUG : 3557/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #24 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #25 of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #26 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3565/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #27 of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3564/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #28 of 4000 documents\n",
      "DEBUG : 3583/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3536/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #29 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3522/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #30 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3563/4000 documents converged within 50 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #31 of 2225 documents\n",
      "DEBUG : performing inference on a chunk of 2225 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 3570/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3540/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 3561/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3530/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3541/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3591/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 1965/2225 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3537/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3539/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3512/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3548/4000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : updating topics\n",
      "INFO : topic #7 (0.020): 0.012*\"food\" + 0.012*\"restaurant\" + 0.009*\"meal\" + 0.009*\"like\" + 0.007*\"serve\" + 0.007*\"try\" + 0.007*\"time\" + 0.006*\"bar\" + 0.006*\"order\" + 0.006*\"family\"\n",
      "INFO : topic #37 (0.020): 0.040*\"salad\" + 0.033*\"good\" + 0.024*\"pasta\" + 0.019*\"italian\" + 0.014*\"sauce\" + 0.014*\"restaurant\" + 0.012*\"food\" + 0.010*\"bread\" + 0.010*\"fresh\" + 0.010*\"delicious\"\n",
      "INFO : topic #2 (0.020): 0.026*\"food\" + 0.021*\"like\" + 0.015*\"good\" + 0.011*\"smell\" + 0.009*\"come\" + 0.009*\"great\" + 0.008*\"time\" + 0.008*\"hair\" + 0.007*\"eat\" + 0.007*\"service\"\n",
      "INFO : topic #14 (0.020): 0.020*\"good\" + 0.019*\"oyster\" + 0.014*\"order\" + 0.012*\"beer\" + 0.010*\"bar\" + 0.010*\"eat\" + 0.008*\"crab\" + 0.008*\"time\" + 0.008*\"fish_chip\" + 0.008*\"try\"\n",
      "INFO : topic #1 (0.020): 0.031*\"bar\" + 0.017*\"crepe\" + 0.014*\"good\" + 0.012*\"place\" + 0.011*\"drink\" + 0.011*\"area\" + 0.010*\"nice\" + 0.008*\"food\" + 0.008*\"great\" + 0.006*\"beer\"\n",
      "INFO : topic diff=0.170368, rho=0.140641\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.518 per-word bound, 183.3 perplexity estimate based on a held-out corpus of 2225 documents with 88742 words\n",
      "INFO : PROGRESS: pass 19, dispatched chunk #0 = documents up to #4000/126225, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 19, dispatched chunk #1 = documents up to #8000/126225, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 19, dispatched chunk #2 = documents up to #12000/126225, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 19, dispatched chunk #3 = documents up to #16000/126225, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 19, dispatched chunk #4 = documents up to #20000/126225, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 19, dispatched chunk #5 = documents up to #24000/126225, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 19, dispatched chunk #6 = documents up to #28000/126225, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 19, dispatched chunk #7 = documents up to #32000/126225, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 19, dispatched chunk #8 = documents up to #36000/126225, outstanding queue size 9\n",
      "INFO : PROGRESS: pass 19, dispatched chunk #9 = documents up to #40000/126225, outstanding queue size 10\n",
      "INFO : PROGRESS: pass 19, dispatched chunk #10 = documents up to #44000/126225, outstanding queue size 11\n",
      "INFO : PROGRESS: pass 19, dispatched chunk #11 = documents up to #48000/126225, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 19, dispatched chunk #12 = documents up to #52000/126225, outstanding queue size 13\n",
      "DEBUG : processing chunk #0 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 19, dispatched chunk #13 = documents up to #56000/126225, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 19, dispatched chunk #14 = documents up to #60000/126225, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 19, dispatched chunk #15 = documents up to #64000/126225, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 19, dispatched chunk #16 = documents up to #68000/126225, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 19, dispatched chunk #17 = documents up to #72000/126225, outstanding queue size 18\n",
      "DEBUG : processing chunk #1 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 19, dispatched chunk #18 = documents up to #76000/126225, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 19, dispatched chunk #19 = documents up to #80000/126225, outstanding queue size 20\n",
      "DEBUG : processing chunk #2 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 19, dispatched chunk #20 = documents up to #84000/126225, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 19, dispatched chunk #21 = documents up to #88000/126225, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 19, dispatched chunk #22 = documents up to #92000/126225, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 19, dispatched chunk #23 = documents up to #96000/126225, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 19, dispatched chunk #24 = documents up to #100000/126225, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 19, dispatched chunk #25 = documents up to #104000/126225, outstanding queue size 26\n",
      "DEBUG : processing chunk #3 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 19, dispatched chunk #26 = documents up to #108000/126225, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 19, dispatched chunk #27 = documents up to #112000/126225, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 19, dispatched chunk #28 = documents up to #116000/126225, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 19, dispatched chunk #29 = documents up to #120000/126225, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 19, dispatched chunk #30 = documents up to #124000/126225, outstanding queue size 31\n",
      "DEBUG : processing chunk #4 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "INFO : PROGRESS: pass 19, dispatched chunk #31 = documents up to #126225/126225, outstanding queue size 32\n",
      "DEBUG : processing chunk #5 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #6 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #7 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #8 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #9 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3607/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3573/4000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #10 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #11 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3585/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3542/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #12 of 4000 documents\n",
      "DEBUG : getting a new job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3557/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3552/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #13 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #14 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3572/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #15 of 4000 documents\n",
      "DEBUG : processing chunk #16 of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3563/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3585/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #17 of 4000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #18 of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3573/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #19 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #20 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3561/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #21 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #22 of 4000 documents\n",
      "DEBUG : 3561/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #23 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3555/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #24 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3604/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #25 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #26 of 4000 documents\n",
      "DEBUG : 3562/4000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #27 of 4000 documents\n",
      "DEBUG : 3585/4000 documents converged within 50 iterations\n",
      "DEBUG : 3572/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #28 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #29 of 4000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : 3553/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3575/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #30 of 4000 documents\n",
      "DEBUG : performing inference on a chunk of 2225 documents\n",
      "DEBUG : performing inference on a chunk of 4000 documents\n",
      "DEBUG : processing chunk #31 of 2225 documents\n",
      "DEBUG : 3585/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3566/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3566/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3605/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3555/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3561/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3545/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 1955/2225 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3601/4000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3584/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3547/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3537/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 3493/4000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : topic #40 (0.020): 0.034*\"great\" + 0.025*\"good\" + 0.019*\"food\" + 0.014*\"nachos\" + 0.014*\"service\" + 0.011*\"experience\" + 0.011*\"server\" + 0.010*\"awesome\" + 0.010*\"delicious\" + 0.009*\"order\"\n",
      "INFO : topic #38 (0.020): 0.039*\"roll\" + 0.020*\"love\" + 0.017*\"menu\" + 0.017*\"delicious\" + 0.016*\"sushi\" + 0.015*\"order\" + 0.015*\"salmon\" + 0.012*\"come\" + 0.011*\"dinner\" + 0.011*\"try\"\n",
      "INFO : topic #2 (0.020): 0.026*\"food\" + 0.021*\"like\" + 0.015*\"good\" + 0.012*\"smell\" + 0.009*\"come\" + 0.009*\"hair\" + 0.008*\"great\" + 0.008*\"time\" + 0.007*\"eat\" + 0.007*\"service\"\n",
      "INFO : topic #12 (0.020): 0.028*\"drink\" + 0.019*\"good\" + 0.015*\"come\" + 0.014*\"order\" + 0.013*\"tea\" + 0.010*\"like\" + 0.010*\"sweet\" + 0.010*\"boba\" + 0.009*\"flavor\" + 0.009*\"place\"\n",
      "INFO : topic #34 (0.020): 0.056*\"vegan\" + 0.016*\"great\" + 0.014*\"place\" + 0.013*\"good\" + 0.010*\"time\" + 0.009*\"food\" + 0.009*\"enjoy\" + 0.008*\"delicious\" + 0.008*\"service\" + 0.008*\"meal\"\n",
      "INFO : topic diff=0.164101, rho=0.139271\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.512 per-word bound, 182.6 perplexity estimate based on a held-out corpus of 2225 documents with 88742 words\n"
     ]
    }
   ],
   "source": [
    "##### Step-3 : Define multicore lda model and enjoy!!!\n",
    "num_topics =50\n",
    "Lda = models.LdaMulticore\n",
    "lda= Lda(doc_term_matrix, num_topics=num_topics,id2word = dictionary, \n",
    "         passes=20,chunksize=4000,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : saving LdaState object under lda_final2.state, separately None\n",
      "DEBUG : {'kw': {}, 'mode': 'wb', 'uri': 'lda_final2.state'}\n",
      "DEBUG : encoding_wrapper: {'errors': 'strict', 'encoding': None, 'mode': 'wb', 'fileobj': <_io.BufferedWriter name='lda_final2.state'>}\n",
      "INFO : saved lda_final2.state\n",
      "DEBUG : {'kw': {}, 'mode': 'wb', 'uri': 'lda_final2.id2word'}\n",
      "DEBUG : encoding_wrapper: {'errors': 'strict', 'encoding': None, 'mode': 'wb', 'fileobj': <_io.BufferedWriter name='lda_final2.id2word'>}\n",
      "INFO : saving LdaMulticore object under lda_final2, separately ['expElogbeta', 'sstats']\n",
      "INFO : storing np array 'expElogbeta' to lda_final2.expElogbeta.npy\n",
      "INFO : not storing attribute dispatcher\n",
      "INFO : not storing attribute id2word\n",
      "INFO : not storing attribute state\n",
      "DEBUG : {'kw': {}, 'mode': 'wb', 'uri': 'lda_final2'}\n",
      "DEBUG : encoding_wrapper: {'errors': 'strict', 'encoding': None, 'mode': 'wb', 'fileobj': <_io.BufferedWriter name='lda_final2'>}\n",
      "INFO : saved lda_final2\n",
      "INFO : saving Dictionary object under dictionary2, separately None\n",
      "DEBUG : {'kw': {}, 'mode': 'wb', 'uri': 'dictionary2'}\n",
      "DEBUG : encoding_wrapper: {'errors': 'strict', 'encoding': None, 'mode': 'wb', 'fileobj': <_io.BufferedWriter name='dictionary2'>}\n",
      "INFO : saved dictionary2\n",
      "INFO : storing corpus in Matrix Market format to doc_term_matrix.mm2\n",
      "DEBUG : {'kw': {}, 'mode': 'wb+', 'uri': 'doc_term_matrix.mm2'}\n",
      "DEBUG : encoding_wrapper: {'errors': 'strict', 'encoding': None, 'mode': 'wb+', 'fileobj': <_io.BufferedRandom name='doc_term_matrix.mm2'>}\n",
      "INFO : saving sparse matrix to doc_term_matrix.mm2\n",
      "INFO : PROGRESS: saving document #0\n",
      "INFO : PROGRESS: saving document #1000\n",
      "INFO : PROGRESS: saving document #2000\n",
      "INFO : PROGRESS: saving document #3000\n",
      "INFO : PROGRESS: saving document #4000\n",
      "INFO : PROGRESS: saving document #5000\n",
      "INFO : PROGRESS: saving document #6000\n",
      "INFO : PROGRESS: saving document #7000\n",
      "INFO : PROGRESS: saving document #8000\n",
      "INFO : PROGRESS: saving document #9000\n",
      "INFO : PROGRESS: saving document #10000\n",
      "INFO : PROGRESS: saving document #11000\n",
      "INFO : PROGRESS: saving document #12000\n",
      "INFO : PROGRESS: saving document #13000\n",
      "INFO : PROGRESS: saving document #14000\n",
      "INFO : PROGRESS: saving document #15000\n",
      "INFO : PROGRESS: saving document #16000\n",
      "INFO : PROGRESS: saving document #17000\n",
      "INFO : PROGRESS: saving document #18000\n",
      "INFO : PROGRESS: saving document #19000\n",
      "INFO : PROGRESS: saving document #20000\n",
      "INFO : PROGRESS: saving document #21000\n",
      "INFO : PROGRESS: saving document #22000\n",
      "INFO : PROGRESS: saving document #23000\n",
      "INFO : PROGRESS: saving document #24000\n",
      "INFO : PROGRESS: saving document #25000\n",
      "INFO : PROGRESS: saving document #26000\n",
      "INFO : PROGRESS: saving document #27000\n",
      "INFO : PROGRESS: saving document #28000\n",
      "INFO : PROGRESS: saving document #29000\n",
      "INFO : PROGRESS: saving document #30000\n",
      "INFO : PROGRESS: saving document #31000\n",
      "INFO : PROGRESS: saving document #32000\n",
      "INFO : PROGRESS: saving document #33000\n",
      "INFO : PROGRESS: saving document #34000\n",
      "INFO : PROGRESS: saving document #35000\n",
      "INFO : PROGRESS: saving document #36000\n",
      "INFO : PROGRESS: saving document #37000\n",
      "INFO : PROGRESS: saving document #38000\n",
      "INFO : PROGRESS: saving document #39000\n",
      "INFO : PROGRESS: saving document #40000\n",
      "INFO : PROGRESS: saving document #41000\n",
      "INFO : PROGRESS: saving document #42000\n",
      "INFO : PROGRESS: saving document #43000\n",
      "INFO : PROGRESS: saving document #44000\n",
      "INFO : PROGRESS: saving document #45000\n",
      "INFO : PROGRESS: saving document #46000\n",
      "INFO : PROGRESS: saving document #47000\n",
      "INFO : PROGRESS: saving document #48000\n",
      "INFO : PROGRESS: saving document #49000\n",
      "INFO : PROGRESS: saving document #50000\n",
      "INFO : PROGRESS: saving document #51000\n",
      "INFO : PROGRESS: saving document #52000\n",
      "INFO : PROGRESS: saving document #53000\n",
      "INFO : PROGRESS: saving document #54000\n",
      "INFO : PROGRESS: saving document #55000\n",
      "INFO : PROGRESS: saving document #56000\n",
      "INFO : PROGRESS: saving document #57000\n",
      "INFO : PROGRESS: saving document #58000\n",
      "INFO : PROGRESS: saving document #59000\n",
      "INFO : PROGRESS: saving document #60000\n",
      "INFO : PROGRESS: saving document #61000\n",
      "INFO : PROGRESS: saving document #62000\n",
      "INFO : PROGRESS: saving document #63000\n",
      "INFO : PROGRESS: saving document #64000\n",
      "INFO : PROGRESS: saving document #65000\n",
      "INFO : PROGRESS: saving document #66000\n",
      "INFO : PROGRESS: saving document #67000\n",
      "INFO : PROGRESS: saving document #68000\n",
      "INFO : PROGRESS: saving document #69000\n",
      "INFO : PROGRESS: saving document #70000\n",
      "INFO : PROGRESS: saving document #71000\n",
      "INFO : PROGRESS: saving document #72000\n",
      "INFO : PROGRESS: saving document #73000\n",
      "INFO : PROGRESS: saving document #74000\n",
      "INFO : PROGRESS: saving document #75000\n",
      "INFO : PROGRESS: saving document #76000\n",
      "INFO : PROGRESS: saving document #77000\n",
      "INFO : PROGRESS: saving document #78000\n",
      "INFO : PROGRESS: saving document #79000\n",
      "INFO : PROGRESS: saving document #80000\n",
      "INFO : PROGRESS: saving document #81000\n",
      "INFO : PROGRESS: saving document #82000\n",
      "INFO : PROGRESS: saving document #83000\n",
      "INFO : PROGRESS: saving document #84000\n",
      "INFO : PROGRESS: saving document #85000\n",
      "INFO : PROGRESS: saving document #86000\n",
      "INFO : PROGRESS: saving document #87000\n",
      "INFO : PROGRESS: saving document #88000\n",
      "INFO : PROGRESS: saving document #89000\n",
      "INFO : PROGRESS: saving document #90000\n",
      "INFO : PROGRESS: saving document #91000\n",
      "INFO : PROGRESS: saving document #92000\n",
      "INFO : PROGRESS: saving document #93000\n",
      "INFO : PROGRESS: saving document #94000\n",
      "INFO : PROGRESS: saving document #95000\n",
      "INFO : PROGRESS: saving document #96000\n",
      "INFO : PROGRESS: saving document #97000\n",
      "INFO : PROGRESS: saving document #98000\n",
      "INFO : PROGRESS: saving document #99000\n",
      "INFO : PROGRESS: saving document #100000\n",
      "INFO : PROGRESS: saving document #101000\n",
      "INFO : PROGRESS: saving document #102000\n",
      "INFO : PROGRESS: saving document #103000\n",
      "INFO : PROGRESS: saving document #104000\n",
      "INFO : PROGRESS: saving document #105000\n",
      "INFO : PROGRESS: saving document #106000\n",
      "INFO : PROGRESS: saving document #107000\n",
      "INFO : PROGRESS: saving document #108000\n",
      "INFO : PROGRESS: saving document #109000\n",
      "INFO : PROGRESS: saving document #110000\n",
      "INFO : PROGRESS: saving document #111000\n",
      "INFO : PROGRESS: saving document #112000\n",
      "INFO : PROGRESS: saving document #113000\n",
      "INFO : PROGRESS: saving document #114000\n",
      "INFO : PROGRESS: saving document #115000\n",
      "INFO : PROGRESS: saving document #116000\n",
      "INFO : PROGRESS: saving document #117000\n",
      "INFO : PROGRESS: saving document #118000\n",
      "INFO : PROGRESS: saving document #119000\n",
      "INFO : PROGRESS: saving document #120000\n",
      "INFO : PROGRESS: saving document #121000\n",
      "INFO : PROGRESS: saving document #122000\n",
      "INFO : PROGRESS: saving document #123000\n",
      "INFO : PROGRESS: saving document #124000\n",
      "INFO : PROGRESS: saving document #125000\n",
      "INFO : PROGRESS: saving document #126000\n",
      "INFO : saved 126225x86671 matrix, density=0.037% (4017025/10940046975)\n",
      "DEBUG : closing doc_term_matrix.mm2\n",
      "DEBUG : closing doc_term_matrix.mm2\n",
      "INFO : saving MmCorpus index to doc_term_matrix.mm2.index\n",
      "DEBUG : {'kw': {}, 'mode': 'wb', 'uri': 'doc_term_matrix.mm2.index'}\n",
      "DEBUG : encoding_wrapper: {'errors': 'strict', 'encoding': None, 'mode': 'wb', 'fileobj': <_io.BufferedWriter name='doc_term_matrix.mm2.index'>}\n"
     ]
    }
   ],
   "source": [
    "lda.save('lda_final2')\n",
    "dictionary.save('dictionary2')\n",
    "corpora.MmCorpus.serialize('doc_term_matrix.mm2', doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
